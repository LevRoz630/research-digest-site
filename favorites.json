{
  "papers": [
    {
      "arxiv_id": "2601.22156v1",
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "link": "https://arxiv.org/abs/2601.22156v1",
      "authors": [
        "Yingfa Chen",
        "Zhen Leng Thai",
        "Zihan Zhou",
        "Zhu Zhang",
        "Xingyu Shen",
        "Shuo Wang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "relevance_score": 9,
      "user_note": "",
      "saved_at": "2026-02-01T20:09:50.002Z"
    }
  ]
}