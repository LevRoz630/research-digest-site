{
  "date": "2026-02-01",
  "generated_at": "2026-02-01T20:46:42.161083+00:00",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "interests": "machine learning, AI agents, large language models",
  "total_papers_fetched": 100,
  "papers": [
    {
      "arxiv_id": "2311.11094",
      "title": "Reinforcement Learning With LLMs Interaction for Distributed Diffusion Model Services",
      "abstract": "Distributed Artificial Intelligence-Generated Content (AIGC) has attracted significant attention, but two key challenges remain: maximizing subjective Quality of Experience (QoE) and improving energy efficiency, which are particularly pronounced in widely adopted Generative Diffusion Model (GDM)-based image generation services. In this paper, we propose a novel user-centric Interactive AI (IAI) approach for service management, with a distributed GDM-based AIGC framework that emphasizes efficient and cooperative deployment. The proposed method restructures the GDM inference process by allowing users with semantically similar prompts to share parts of the denoising chain. Furthermore, to maximize the users\u2019 subjective QoE, we propose an IAI approach, i.e., Reinforcement Learning With Large Language Models Interaction (RLLI), which utilizes Large Language Model (LLM)-empowered generative agents to replicate users interactions, providing real-time and subjective QoE feedback aligned with diverse user personalities. Lastly, we present the GDM-based Deep Deterministic Policy Gradient (G-DDPG) algorithm, adapted to the proposed RLLI framework, to allocate communication and computing resources effectively while accounting for subjective user traits and dynamic wireless conditions. Simulation results demonstrate that G-DDPG improves total QoE by 15% compared with the standard DDPG algorithm.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Medicine"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/ee91a988e05badea474468b0dcd4bc02914defb1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses topics of machine learning, AI agents, and large language models, making it highly relevant to a researcher with those interests.",
      "author_h_indices": [
        43,
        16,
        114,
        56,
        68,
        9,
        18,
        20
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.394897959183673
    },
    {
      "arxiv_id": "2304.07297",
      "title": "Language Instructed Reinforcement Learning for Human-AI Coordination",
      "abstract": "One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination performance in human evaluations in Hanabi.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/bd05f81167ca3f77460f4a1da3bf5ade9febb15b",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it specifically addresses the use of large language models in reinforcement learning for human-AI coordination, which aligns with the researcher's interests in machine learning, AI agents, and language models.",
      "author_h_indices": [
        16,
        64
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.36734693877551
    },
    {
      "arxiv_id": "2405.17631",
      "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
      "abstract": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/bba1d4de76b8330bea5f73ddeb99da4e01c02e16",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper discusses an AI agent for designing experiments, which aligns with interests in AI agents and machine learning. Additionally, the use of large language models in accelerating scientific discovery is also related to the interest in large language models.",
      "author_h_indices": [
        16,
        3,
        5,
        1,
        1,
        8,
        147
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.237463556851312
    },
    {
      "arxiv_id": "2402.01622",
      "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
      "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/11155af5ccd1889277f4269f6bb349a7633554f4",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper explores the capabilities of language agents powered by large language models in the context of planning, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        13,
        23,
        20,
        5,
        13,
        1,
        13,
        42
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.14923469387755
    },
    {
      "arxiv_id": "2402.05119",
      "title": "A Closer Look at the Limitations of Instruction Tuning",
      "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/acbce5ebf3f254188d10f6ba7de1ba716db89774",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it delves into the limitations of instruction tuning for training large language models, which aligns with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        18,
        10,
        2,
        15,
        10,
        12,
        2,
        56,
        20
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.14795918367347
    },
    {
      "arxiv_id": "2504.14110",
      "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks",
      "abstract": "Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five\"AI-dreamt\"MOFs, representing a major step toward automated synthesizable material discovery.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/11ac1691fa7415da4f7727d74244c7d9b3ba6fce",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper relates to the use of large language models and AI agents for material discovery, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        1,
        4,
        2,
        1,
        11,
        1,
        10,
        2,
        15,
        55,
        56,
        59,
        2,
        12
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.145102040816328
    },
    {
      "arxiv_id": "2508.06569",
      "title": "Operationalizing Serendipity: Multi-Agent AI Workflows for Enhanced Materials Characterization with Theory-in-the-Loop",
      "abstract": "The history of science is punctuated by serendipitous discoveries, where unexpected observations, rather than targeted hypotheses, opened new fields of inquiry. While modern autonomous laboratories excel at accelerating hypothesis testing, their optimization for efficiency risks overlooking these crucial, unplanned findings. To address this gap, we introduce SciLink, an open-source, multi-agent artificial intelligence framework designed to operationalize serendipity in materials research by creating a direct, automated link between experimental observation, novelty assessment, and theoretical simulations. The framework employs a hybrid AI strategy where specialized machine learning models perform quantitative analysis of experimental data, while large language models handle higher-level reasoning. These agents autonomously convert raw data from materials characterization techniques into falsifiable scientific claims, which are then quantitatively scored for novelty against the published literature. We demonstrate the framework's versatility across diverse research scenarios, showcasing its application to atomic-resolution and hyperspectral data, its capacity to integrate real-time human expert guidance, and its ability to close the research loop by proposing targeted follow-up experiments. By systematically analyzing all observations and contextualizing them, SciLink provides a practical framework for AI-driven materials research that not only enhances efficiency but also actively cultivates an environment ripe for serendipitous discoveries, thereby bridging the gap between automated experimentation and open-ended scientific exploration.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/5b014f349a093bc770a18858d8afbb21290bfce3",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of the researcher by discussing multi-agent AI workflows, machine learning, and large language models in the context of materials characterization.",
      "author_h_indices": [
        1,
        9,
        1,
        14,
        68,
        3,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.127259475218658
    },
    {
      "arxiv_id": "2501.06706",
      "title": "AIOpsLab: A Holistic Framework to Evaluate AI Agents for Enabling Autonomous Clouds",
      "abstract": "AI for IT Operations (AIOps) aims to automate complex operational tasks, such as fault localization and root cause analysis, to reduce human workload and minimize customer impact. While traditional DevOps tools and AIOps algorithms often focus on addressing isolated operational tasks, recent advances in Large Language Models (LLMs) and AI agents are revolutionizing AIOps by enabling end-to-end and multitask automation. This paper envisions a future where AI agents autonomously manage operational tasks throughout the entire incident lifecycle, leading to self-healing cloud systems, a paradigm we term AgentOps. Realizing this vision requires a comprehensive framework to guide the design, development, and evaluation of these agents. To this end, we present AIOPSLAB, a framework that not only deploys microservice cloud environments, injects faults, generates workloads, and exports telemetry data but also orchestrates these components and provides interfaces for interacting with and evaluating agents. We discuss the key requirements for such a holistic framework and demonstrate how AIOPSLAB can facilitate the evaluation of next-generation AIOps agents. Through evaluations of state-of-the-art LLM agents within the benchmark created by AIOPSLAB, we provide insights into their capabilities and limitations in handling complex operational tasks in cloud environments.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/c8bbe39285da8e427250cc2a80da8f8f2f92b8cb",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests in machine learning, AI agents, and large language models, focusing on enhancing AIOps through the use of AI agents in cloud environments.",
      "author_h_indices": [
        4,
        3,
        10,
        14,
        43,
        2,
        12,
        6,
        28
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.124489795918366
    },
    {
      "arxiv_id": "2509.01398",
      "title": "The Need for Verification in AI-Driven Scientific Discovery",
      "abstract": "Artificial intelligence (AI) is transforming the practice of science. Machine learning and large language models (LLMs) can generate hypotheses at a scale and speed far exceeding traditional methods, offering the potential to accelerate discovery across diverse fields. However, the abundance of hypotheses introduces a critical challenge: without scalable and reliable mechanisms for verification, scientific progress risks being hindered rather than being advanced. In this article, we trace the historical development of scientific discovery, examine how AI is reshaping established practices for scientific discovery, and review the principal approaches, ranging from data-driven methods and knowledge-aware neural architectures to symbolic reasoning frameworks and LLM agents. While these systems can uncover patterns and propose candidate laws, their scientific value ultimately depends on rigorous and transparent verification, which we argue must be the cornerstone of AI-assisted discovery.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/6fce1cef9f8e681a1827af835e5cbb19f5c92538",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the role of machine learning and AI agents, including large language models, in scientific discovery, highlighting the importance of verification in the process.",
      "author_h_indices": [
        1,
        3,
        9,
        25,
        28
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.121224489795917
    },
    {
      "arxiv_id": "2503.03983",
      "title": "Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding and Expert Reasoning Abilities",
      "abstract": "Understanding and reasoning over non-speech sounds and music are crucial for both humans and AI agents to interact effectively with their environments. In this paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM) with advanced audio understanding and reasoning capabilities. AF2 leverages (i) a custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio reasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves state-of-the-art performance with only a 3B parameter small language model, surpassing large open-source and proprietary models across over 20 benchmarks. Next, for the first time, we extend audio understanding to long audio segments (30 secs to 5 mins) and propose LongAudio, a large and novel dataset for training ALMs on long audio captioning and question-answering tasks. Fine-tuning AF2 on LongAudio leads to exceptional performance on our proposed LongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio understanding capabilities. We conduct extensive ablation studies to confirm the efficacy of our approach. Project Website: https://research.nvidia.com/labs/adlr/AF2/.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Engineering"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/b4ef23c8662afc319e7fa85f76a88ac9e06750a3",
      "relevance_score": 9.0,
      "relevance_reason": "The paper directly addresses topics related to machine learning, AI agents, and large language models, specifically focusing on an Audio-Language Model with advanced audio understanding and reasoning capabilities.",
      "author_h_indices": [
        18,
        8,
        15,
        7,
        2,
        17,
        7,
        20,
        29,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.113877551020408
    },
    {
      "arxiv_id": "s2:5fdca75da86e4332257bb1067bc3eabb3d06bb34",
      "title": "A Scoping Review of AI-Driven Digital Interventions in Mental Health Care: Mapping Applications Across Screening, Support, Monitoring, Prevention, and Clinical Education",
      "abstract": "Background/Objectives: Artificial intelligence (AI)-enabled digital interventions are increasingly used to expand access to mental health care. This PRISMA-ScR scoping review maps how AI technologies support mental health care across five phases: pre-treatment (screening), treatment (therapeutic support), post-treatment (monitoring), clinical education, and population-level prevention. Methods: We synthesized findings from 36 empirical studies published through January 2024 that implemented AI-driven digital tools, including large language models (LLMs), machine learning (ML) models, and conversational agents. Use cases include referral triage, remote patient monitoring, empathic communication enhancement, and AI-assisted psychotherapy delivered via chatbots and voice agents. Results: Across the 36 included studies, the most common AI modalities included chatbots, natural language processing tools, machine learning and deep learning models, and large language model-based agents. These technologies were predominantly used for support, monitoring, and self-management purposes rather than as standalone treatments. Reported benefits included reduced wait times, increased engagement, and improved symptom tracking. However, recurring challenges such as algorithmic bias, data privacy risks, and workflow integration barriers highlight the need for ethical design and human oversight. Conclusion: By introducing a four-pillar framework, this review offers a comprehensive overview of current applications and future directions in AI-augmented mental health care. It aims to guide researchers, clinicians, and policymakers in developing safe, effective, and equitable digital mental health interventions.",
      "authors": [],
      "categories": [
        "Medicine"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/5fdca75da86e4332257bb1067bc3eabb3d06bb34",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper explores AI-driven digital interventions, including large language models and machine learning, in the context of mental health care.",
      "author_h_indices": [
        2,
        18
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.091836734693876
    },
    {
      "arxiv_id": "2311.09835",
      "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
      "abstract": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution. Our code, dataset, and models are available at https://github.com/gersteinlab/ML-bench.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/f537640c4e55d2a794a0af3ec60651f3820c702e",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper specifically addresses the performance evaluation of Large Language Models and AI agents on real-world code repositories, which aligns well with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        5,
        28,
        14,
        3,
        8,
        10,
        3,
        4,
        2,
        9,
        2,
        10,
        2,
        10,
        4,
        7,
        5,
        3,
        9,
        3,
        13,
        28,
        9,
        26,
        22,
        19
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.091130298273155
    },
    {
      "arxiv_id": "2502.14499",
      "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
      "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/e87d34032cc2a77a7711a651296a8f51c9474929",
      "relevance_score": 9.0,
      "relevance_reason": "Highly relevant as the paper introduces a new framework and benchmark for evaluating and developing large language models (LLMs) agents on AI research tasks, which align with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        8,
        9,
        2,
        8,
        5,
        1,
        2,
        9,
        2,
        1,
        26,
        7,
        5,
        2,
        41,
        2,
        28
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.085354141656664
    },
    {
      "arxiv_id": "2402.16823",
      "title": "Language Agents as Optimizable Graphs",
      "abstract": "Various human-designed prompt engineering techniques have been proposed to improve problem solvers based on Large Language Models (LLMs), yielding many disparate code bases. We unify these approaches by describing LLM-based agents as computational graphs. The nodes implement functions to process multimodal data or query LLMs, and the edges describe the information flow between operations. Graphs can be recursively combined into larger composite graphs representing hierarchies of inter-agent collaboration (where edges connect operations of different agents). Our novel automatic graph optimizers (1) refine node-level LLM prompts (node optimization) and (2) improve agent orchestration by changing graph connectivity (edge optimization). Experiments demonstrate that our framework can be used to efficiently develop, integrate, and automatically improve various LLM agents. The code can be found at https://github.com/metauto-ai/gptswarm.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/4580fa51ab097a56b98ed1c820f6547c145d6ca4",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of a researcher focused on machine learning, AI agents, and large language models by proposing a framework for optimizing and improving LLM-based agents using computational graphs.",
      "author_h_indices": [
        10,
        5,
        11,
        10,
        6,
        8
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.076530612244898
    },
    {
      "arxiv_id": "2311.08166",
      "title": "MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge",
      "abstract": "Solving mechanics problems using numerical methods requires comprehensive intelligent capability of retrieving relevant knowledge and theory, constructing and executing codes, analyzing the results, a task that has thus far mainly been reserved for humans. While emerging AI methods can provide effective approaches to solve end-to-end problems, for instance via the use of deep surrogate models or various data analytics strategies, they often lack physical intuition since knowledge is baked into the parametric complement through training, offering less flexibility when it comes to incorporating mathematical or physical insights. By leveraging diverse capabilities of multiple dynamically interacting large language models (LLMs), we can overcome the limitations of conventional approaches and develop a new class of physics-inspired generative machine learning platform, here referred to as MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for elasticity problems, via autonomous collaborations. A two-agent team can effectively write, execute and self-correct code, in order to apply finite element methods to solve classical elasticity problems in various flavors (different boundary conditions, domain geometries, meshes, small/finite deformation and linear/hyper-elastic constitutive laws, and others). For more complex tasks, we construct a larger group of agents with enhanced division of labor among planning, formulating, coding, executing and criticizing the process and results. The agents mutually correct each other to improve the overall team-work performance in understanding, formulating and validating the solution. Our framework shows the potential of synergizing the intelligence of language models, the reliability of physics-based modeling, and the dynamic collaborations among diverse agents, opening novel avenues for automation of solving engineering problems.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/da17ec7b2867c3e02951c5598936b891ebe865ce",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of the researcher by discussing machine learning, AI agents, and large language models in the context of solving mechanics problems and generating new data.",
      "author_h_indices": [
        5,
        11
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.073469387755104
    },
    {
      "arxiv_id": "2408.06292",
      "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
      "abstract": "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/33161a5a9b5dcb635b5a97475e6a6209a69ada7d",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it introduces a framework for fully automated scientific discovery using frontier large language models in the context of machine learning research.",
      "author_h_indices": [
        5,
        14,
        8,
        9,
        5,
        3
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.06734693877551
    },
    {
      "arxiv_id": "s2:7b1690c201e5c7ffe6333f924b75a68a9a02a4f6",
      "title": "Integrating Fuzzy Evaluation Agents and LLM-Based Robots for Multilingual Interactive Applications",
      "abstract": "This paper proposes an integrated framework that combines fuzzy evaluation agents and LLM-based language-specific robots to support multilingual interactive learning with humancentered AI (HAI) applications. The core techniques of the LLMbased intelligent fuzzy system include: (1) a multi-modal interactive learning mechanism through human and machine co-learning based on HAI with fuzzy knowledge graph (FKG), (2) a real-time cross languages processing agent with quantum fuzzy inference engine for Taiwanese, English, Japanese and Chinese languages, and (3) a fuzzy retrieval-augmented generation (RAG) mechanism that integrates fuzzy semantic search with a large language model (LLM) reasoning based on Llama 3-TAIDE-70B and Gemma 3-TAIDE-12B-Chat models. The proposed framework is applied to multilingual human-machine co-learning scenarios, where both human learners and LLM-based robots engage in interactive applications. Within this setting, the fuzzy evaluation agents and LLM-based robots collaboratively analyze student learning performance, based on data collected from the co-learning environment and quantum fuzzy inference engine. Experimental results demonstrate that the proposed method is effective and feasible for supporting multilingual, human-centered interactive learning environments.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/7b1690c201e5c7ffe6333f924b75a68a9a02a4f6",
      "relevance_score": 9.0,
      "relevance_reason": "The paper directly addresses the integration of AI agents and large language models in a multilingual interactive learning framework, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        24,
        22,
        0,
        0,
        4,
        0,
        5,
        2
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.065433673469387
    },
    {
      "arxiv_id": "2402.04268",
      "title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
      "abstract": "Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data \u2013 natural vibrational frequencies \u2013 via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.",
      "authors": [],
      "categories": [
        "Physics",
        "Computer Science",
        "Biology",
        "Medicine"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/797597040167622a126c3206bbf94459238e2c19",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of a researcher focused on machine learning, AI agents, and large language models in the context of protein discovery and design.",
      "author_h_indices": [
        6,
        7
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.05969387755102
    },
    {
      "arxiv_id": "2311.18232",
      "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
      "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/1e672bf4d38a93c4c140ee208216425444368fa6",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on using reinforcement learning with large language models to create goal-directed language agents for multi-turn interactions, aligning closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        1,
        9,
        2,
        3,
        18,
        3,
        10
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.05969387755102
    },
    {
      "arxiv_id": "2407.10022",
      "title": "AtomAgents: Alloy design and discovery through physics-aware multi-modal multi-agent artificial intelligence",
      "abstract": "The design of alloys is a multi-scale problem that requires a holistic approach that involves retrieving relevant knowledge, applying advanced computational methods, conducting experimental validations, and analyzing the results, a process that is typically reserved for human experts. Machine learning (ML) can help accelerate this process, for instance, through the use of deep surrogate models that connect structural features to material properties, or vice versa. However, existing data-driven models often target specific material objectives, offering limited flexibility to integrate out-of-domain knowledge and cannot adapt to new, unforeseen challenges. Here, we overcome these limitations by leveraging the distinct capabilities of multiple AI agents that collaborate autonomously within a dynamic environment to solve complex materials design tasks. The proposed physics-aware generative AI platform, AtomAgents, synergizes the intelligence of large language models (LLM) the dynamic collaboration among AI agents with expertise in various domains, including knowledge retrieval, multi-modal data integration, physics-based simulations, and comprehensive results analysis across modalities that includes numerical data and images of physical simulation results. The concerted effort of the multi-agent system allows for addressing complex materials design problems, as demonstrated by examples that include autonomously designing metallic alloys with enhanced properties compared to their pure counterparts. Our results enable accurate prediction of key characteristics across alloys and highlight the crucial role of solid solution alloying to steer the development of advanced metallic alloys. Our framework enhances the efficiency of complex multi-objective design tasks and opens new avenues in fields such as biomedical materials engineering, renewable energy, and environmental sustainability.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/741d039aba804db2e2600fc7be7a1b8e303aec49",
      "relevance_score": 9.0,
      "relevance_reason": "The paper directly discusses the use of machine learning, AI agents, and large language models in the design of alloys, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        7
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.05969387755102
    },
    {
      "arxiv_id": "2503.22164",
      "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
      "abstract": "The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Biology"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/46e921cb6fba3423e48fa894afa9b0b47bf934d6",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on the use of large language models in the context of AI agents and drug discovery, which aligns well with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        6,
        2,
        3,
        9,
        3,
        12
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.053790087463558
    },
    {
      "arxiv_id": "s2:6bd18e556a1884ef5681867d44fd044dab39d424",
      "title": "Fine Tuning LLMs vs Non-Generative Machine Learning Models: A Comparative Study of Malware Detection",
      "abstract": ": The emergence of Generative AI has provided various scenarios where Large Language Models can be used to replace older technologies. Cyber-security industry has been an early adopter of these technologies, but in particular for scenarios that involved security operation centers, support or cyber attack visibility. This paper aims to compare how well Large Language Models behave against traditional machine learning models for malware detection wrt. various constrains that apply to a security product such as inference time, memory footprint, detection and false positive rate. In this paper we have fine tuned 3 open source models (LLama2-13B, Mistral, Mixtral) and compared them with 18 classical machine learning models (feed forward neural networks, SVMs, etc) using more than 135,000 benign and malicious binary samples. The goal was to identify scenarios/cases where large language models are suited for the task of malware detection.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/6bd18e556a1884ef5681867d44fd044dab39d424",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly compares Large Language Models with traditional machine learning models in the context of malware detection, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        4,
        2,
        11
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.052040816326532
    },
    {
      "arxiv_id": "2408.14033",
      "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents",
      "abstract": "Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/d0cd8b45949b959c316a3ed75a4683d0a70b1aa9",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on autonomous machine learning research utilizing large language model agents, which directly aligns with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        2,
        13,
        1,
        5
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.049591836734693
    },
    {
      "arxiv_id": "2410.02958",
      "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML",
      "abstract": "Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/efde8940a0b924e93d35184c4a1e8f9670b94fe7",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on AutoML, AI agents, and large language models within the context of a multi-agent framework for full-pipeline AutoML. It addresses the automation of tasks in the development pipeline and the use of LLMs to increase usability, which aligns well with the researcher's interests in machine learning, AI agents, and large language models.",
      "author_h_indices": [
        6,
        8,
        2
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.048979591836734
    },
    {
      "arxiv_id": "2501.07278",
      "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
      "abstract": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Medicine"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/76aebf01bdfeaf743ac83ac231384a861f7b69ca",
      "relevance_score": 9.0,
      "relevance_reason": "Direct match to interests in machine learning, AI agents, and large language models. The paper discusses lifelong learning techniques for LLM-based agents, which aligns closely with the researcher's interests.",
      "author_h_indices": [
        11,
        2,
        3,
        2,
        5,
        4,
        5,
        8
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.045918367346939
    }
  ]
}