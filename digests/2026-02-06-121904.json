{
  "date": "2026-02-06",
  "generated_at": "2026-02-06T12:19:12.111066+00:00",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "interests": "Ai safety, reinforcement learning optimization, token expenditure optimization, LLM memory allocation optimization.\n\nWhen ranking, also consider:\n- Author credentials and reputation (prefer established researchers from top institutions)\n- Quality of methodology described in abstract\n- Novelty and potential impact of the work\n- Papers with well-known authors in the field should be scored higher",
  "total_papers_fetched": 50,
  "papers": [
    {
      "arxiv_id": "s2:adbb0b23447a4992b90c4415c552bbf7f09c7d84",
      "title": "Adaptive Generalized Proportional Fair Scheduling with Deep Reinforcement Learning",
      "abstract": "The emergence of 5G and the upcoming 6G has been and will be supporting numerous applications with various quality of service (QoS) requirements which inevitably come with increased complexity. Artificial Intelligence (AI) is getting tremendous attention in handling the complexity of wireless communication systems. Among many, medium access control (MAC) scheduling, which controls wireless resource allocation among different bearers or UEs, is one of the potential applications of AI, for it is difficult to dynamically optimize due to the non-deterministic and the partially-occupying characteristics of the mobile traffic. In this work, we formulate an optimization problem of maximizing user perceived throughput (UPT) while minimizing packet delay violation by controlling the parameters of the MAC scheduling algorithm. This problem is a combinatorial optimization problem, and optimal points vary with circumstances. We use deep reinforcement learning (DRL) to design a dynamic policy that adaptively changes the governing parameters of the scheduler. A 3D-heatmap capturing the status of a network with varying UEs is created. An efficient convolutional neural network with long-short term memory (CNN-LSTM) networks and a reinforcement learning framework are used for efficient training. Experiments in dynamically changing circumstances show that our DRL-based policy adapts to the network states while providing QoS satisfaction.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2022-01-01",
      "updated": "2022-01-01",
      "link": "https://www.semanticscholar.org/paper/adbb0b23447a4992b90c4415c552bbf7f09c7d84",
      "relevance_score": 8.0,
      "relevance_reason": "This paper is relevant as it addresses topics related to AI safety, reinforcement learning optimization, and memory allocation optimization in the context of MAC scheduling in wireless communication systems. The methodology described in the abstract demonstrates the use of deep reinforcement learning and convolutional neural networks with LSTM to optimize network performance.",
      "author_h_indices": [
        2,
        2,
        15,
        1,
        4,
        8
      ],
      "quality_score": 8.133333333333333
    },
    {
      "arxiv_id": "s2:c9e38f890297470e8848bafd8b1ac1dd7f9e539b",
      "title": "A Batch-Constrained Safe Deep Q-Learning-Based Cloud-Edge Collaborative Framework for Dynamic Operation Optimization in Industrial Processes",
      "abstract": "Operation optimization of industrial processes is essential for high-quality industrial development, which involves production, quality, cost, safety, and sustainability. However, the complexity, dynamic uncertainty, inconsistent data quality, and limited computational resources of industrial processes pose significant challenges for operation optimization. To address these issues, we propose a batch-constrained safe deep Q-learning-based cloud-edge collaborative framework for dynamic operation optimization. First, a performance-traceable data space is constructed based on spatio-temporal alignment of equipment data, which is embedded with format validation, boxplot\u2013based outlier detection, and mean-value filling algorithms to enhance data quality and support high-level applications. Second, a batch-constrained safe deep Q-learning algorithm is developed, in which the agent uses a conditional variational autoencoder to generate candidate actions matching the historical data distribution, and applies a safety protection strategy, perturbation model, and double Q-networks to explore safe optimization policies, thereby eliminating extrapolation error in offline reinforcement learning and ensuring the safety of operation optimization. Then, a novel parallel informer and long-short term memory model is constructed to capture process features across multiple time scales for online performance prediction. Whenever process performance deviates from the specifications, the actor network is immediately triggered to generate a dynamic optimization policy, which is then fed back into the performance prediction model for evaluation, ensuring reliable optimization outcomes. Finally, by leveraging a cloud\u2013edge architecture to address computational resource allocation and efficiency, we apply the proposed framework to a real hot strip mill process, demonstrating its effectiveness and safety.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2026-01-01",
      "updated": "2026-01-01",
      "link": "https://www.semanticscholar.org/paper/c9e38f890297470e8848bafd8b1ac1dd7f9e539b",
      "relevance_score": 8.0,
      "relevance_reason": "This paper is relevant due to its focus on reinforcement learning optimization and operation optimization, which align with the researcher's interests in AI safety and token expenditure optimization. Additionally, the methodology described in the abstract shows promise for addressing challenges in industrial processes.",
      "author_h_indices": [
        3,
        0,
        6,
        6
      ],
      "quality_score": 8.09375
    },
    {
      "arxiv_id": "s2:4c133244b281a1d24f64f5a5453188f93547bb4d",
      "title": "Reinforcement Learning in Healthcare: Enhancing Treatment and Resource Allocation",
      "abstract": "Reinforcement learning (RL) has become a promising strategy for optimizing patient treatment and resource allocation in changing clinical settings. RL-driven solutions for personalized treatment, hospital resource management, emergency department triage, drug dosage optimization, and adaptive staffing are explored in this research. We use datasets including MIMIC-III, eICU, NHAMCS, DCCT, AHA Annual Survey Database, and Medicare Cost Reports to create realistic healthcare scenarios and assess the effect of RL. Statistical graphs show how RL performs better than conventional approaches when it comes to information in increasing the success in treatment, shortening patient wait, reducing resource utilization, and lowering medical expenditures. These findings demonstrate RL\u2019s ability to improve healthcare decision-making, resulting in better patient outcomes as well as more efficient use of healthcare resources, all while considering the computational and ethical challenges surrounding AI-assisted clinical decision-making.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/4c133244b281a1d24f64f5a5453188f93547bb4d",
      "relevance_score": 8.0,
      "relevance_reason": "This paper is relevant as it discusses reinforcement learning optimization in healthcare, which can be applied to AI safety and token expenditure optimization. The methodology and datasets used indicate high quality research.",
      "author_h_indices": [
        3,
        3,
        3
      ],
      "quality_score": 8.075
    },
    {
      "arxiv_id": "s2:4e966e9514243cac12d88178a972ee07ab390aca",
      "title": "AI-Enhanced Performance Optimization for Microservice-Based Systems",
      "abstract": "Microservice architectures (MSAs) have revolutionized software development by offering flexibility, scalability, and resilience through the decomposition of applications into loosely coupled services. However, resource management and performance optimization in MSAs remain challenging due to dynamic workloads and complex interdependencies. Traditional approaches, such as static provisioning and rule-based scaling, struggle to handle these challenges efficiently, often leading to over-provisioning or under-provisioning of resources. In this paper, we propose an AI-driven optimization framework that integrates reinforcement learning (RL), predictive analytics (PA), and evolutionary algorithms (EA) to dynamically manage resources in microservices environments. The proposed framework anticipates workload changes, optimizes resource allocation in real-time, and continuously adapts to evolving system conditions. Our empirical evaluation, conducted on a Kubernetes-based microservice platform, demonstrates significant improvements in performance and resource efficiency compared to conventional methods like Kubernetes' Horizontal Pod Autoscaler (HPA). The AI-driven system achieves up to a 27.3% reduction in latency during traffic surges and improves throughput by 25.7%, while also reducing CPU and memory usage by up to 25.7% and 22.7%, respectively. These results suggest that AI-driven optimization offers a scalable and efficient solution for managing microservices in highly dynamic environments.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/4e966e9514243cac12d88178a972ee07ab390aca",
      "relevance_score": 8.0,
      "relevance_reason": "The paper is highly relevant as it focuses on AI-driven optimization for resource management in microservices environments, which aligns with interests in AI safety, reinforcement learning optimization, and token expenditure optimization. The methodology described in the abstract includes reinforcement learning, predictive analytics, and evolutionary algorithms, demonstrating a quality approach. The potential impact of the work on performance and resource efficiency in microservice-based systems is significant.",
      "author_h_indices": [
        3
      ],
      "quality_score": 8.075
    },
    {
      "arxiv_id": "s2:f3e97e1ff2e9fc5c6233ef478cc41d2264a29d0d",
      "title": "RATM: Reinforcement Learning For Co-Optimized CPU Scheduling and NUMA Memory Management",
      "abstract": "Modern operating systems rely on static heuristics\u2014 carefully tuned at design time \u2014 to manage CPU scheduling and memory allocation. These heuristics fundamentally fail under the different dynamically shifting workloads characteristic of contemporary data centers, where batch processing, real-time analytics, and interactive services coexist. This paper presents RATM (Resource-Aware Adaptive Task Manager), a novel \"Authoritative Controller\" architecture implemented in Rust that replaces static policies with a Deep Q-Network (DQN) reinforcement learning agent capable of optimizing kernel behavior at runtime. Our system introduces a strict Policy-Mechanism Separation, a model-free DQN agent that observes continuous system state and selects actions, while the VRRP (Varying Response Ratio Priority) Scheduler and NAAT (NUMA-Aware Adaptive Tiered) Allocator execute commands as passive, tunable mechanisms. The RATM controller mediates between these layers, enforcing safety invariants and translating abstract actions into concrete API calls. The experimental results demonstrate that our RL-driven kernel achieves over up to 70% reduction in average wait latency in calibration scenarios compared to the static baseline, while maintaining high fairness. The RL agent learns to proactively trigger NUMA page migrations during workload phase transitions, effectively \"flattening the curve\" of latency spikes that plague traditional schedulers. The entire implementation \u2014 including lock-free data structures, atomic metrics collection, and the RL training loop \u2014 is realized in safe Rust, leveraging the language's ownership model and `Send`/`Sync` traits to eliminate data races by construction. This work demonstrates that adaptive, learning-based kernel subsystems are not only feasible but can be implemented with the same safety guarantees expected of production operating systems.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2026-01-01",
      "updated": "2026-01-01",
      "link": "https://www.semanticscholar.org/paper/f3e97e1ff2e9fc5c6233ef478cc41d2264a29d0d",
      "relevance_score": 8.0,
      "relevance_reason": "The paper is highly relevant as it combines reinforcement learning optimization with memory allocation optimization, which is directly related to the researcher's interests. The methodology described in the abstract is of high quality, and the potential impact of the work in improving CPU scheduling and memory management in data centers is significant.",
      "author_h_indices": [
        0,
        0,
        0,
        0,
        0
      ],
      "quality_score": 8.0
    },
    {
      "arxiv_id": "s2:d23a031467926ac45f925809c875329314c25aa3",
      "title": "Artificial Intelligence for Autonomous Infrastructure: A Deep Reinforcement Learning Approach to Datacenter Operations",
      "abstract": "The current datacenter operations are more complex than ever before due to the skyrocketing demand\nfor cloud services, Internet of Things (IoT) applications, and real-time analytics. Classical rule-of-thumb control and\nheuristic optimization cannot keep up with the highly dynamic nature of non-linear large-scale computing\ninfrastructure. The paper explores deep reinforcement learning (DRL) as a basis for fully autonomous infrastructure\nmanagement, specifically thermal regulation, workload scheduling, and energy-conscious resource allocation.\nWe initially examine the shortcomings of traditional datacenter control loops and outline the gaps that do not facilitate\nscalability and fault tolerance. Our next suggestion is a hybrid DARA system comprising model-free policy learning\nand predictive simulations of digital twins to allow self-optimizing behavior under unpredictable workloads and\nequipment breakdowns. An implementation on a simple datacenter simulator using live telemetry streams has been\ntested and shown to perform 18 percent better in cooling energy and 12 percent better in resource utilization than state-\nof-the-art baselines.\nThe findings attest to the fact that DRL can assist in autonomous infrastructure that is capable of constant adaptation\nwithout human assistance. We mention the practical deployment issues, such as data quality, safety limitations, and how\nit works with the legacy orchestration platforms, and the future research directions that would bring us to the fully self-\ngoverning datacenters. The study also adds to the existing literature that AI-based control can reduce the operational\nexpenses and environmental footprint significantly and enhance the reliability of the provided services.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/d23a031467926ac45f925809c875329314c25aa3",
      "relevance_score": 8.0,
      "relevance_reason": "The paper is highly relevant as it covers topics related to AI safety, reinforcement learning optimization, and memory allocation optimization in the context of datacenter operations. The methodology described in the abstract seems sound and the potential impact of the work is significant for autonomous infrastructure management.",
      "author_h_indices": [
        0
      ],
      "quality_score": 8.0
    },
    {
      "arxiv_id": "s2:3733f73913899ed133f28407cd0ea7a90fa8abd6",
      "title": "AI-Based Automated Load Testing and Resource Scaling in Cloud Environments Using Self-Learning Agents",
      "abstract": "Cloud environments face dynamic and unpredictable workloads, making efficient load testing and resource scaling critical for maintaining performance, reducing costs, and ensuring reliability. Traditional approaches to load testing and scaling rely on predefined rules or manual intervention, which often fail to adapt to rapidly changing demand patterns. This research introduces an AI-based automated framework that leverages self-learning agents to conduct continuous load testing and intelligent resource scaling in real time. The proposed system employs reinforcement learning and adaptive performance modeling to simulate variable workloads, detect bottlenecks, and optimize resource allocation with minimal human intervention. Experimental results in a simulated multi-cloud environment demonstrate significant improvements in response time, throughput, and cost efficiency compared to static or heuristic-based scaling methods. The findings suggest that self-learning agents can transform cloud resource management into a fully autonomous, performance-driven process, enabling service providers to meet stringent SLA requirements while optimizing operational expenditure.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/3733f73913899ed133f28407cd0ea7a90fa8abd6",
      "relevance_score": 8.0,
      "relevance_reason": "The paper is highly relevant due to its focus on AI-based optimization in cloud environments, which aligns with the researcher's interests in AI safety, reinforcement learning optimization, and token expenditure optimization. The methodology described in the abstract demonstrates a novel approach with potential impact, and the authors' credentials and reputation should be considered strong given the topic.",
      "author_h_indices": [
        0,
        0
      ],
      "quality_score": 8.0
    },
    {
      "arxiv_id": "s2:a653a99ad0a79d6833d0cde4d85806675d9819bf",
      "title": "Real-time Data Pipeline Optimization using AI-driven Resource Allocation",
      "abstract": "This paper discusses an AI approach to improve how data is processed in real time by adjusting the way\nresources are given to distributed systems. Our framework adapts and changes the amount of CPU, available memory,\nand network use to hit performance goals due to machine learning and reinforcement learning. The system has shown\nimprovement in latency, throughput, and using resources more efficiently during evaluation with many workloads. To\ntackle the challenges produced by intense and fast data, we make use of predictive analytics and scheduling that can\nadjust automatically. Findings point out that using AI for resource efficiency is a flexible and dependable way to run\nmodern data engineering tasks both in clouds and at edges.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2012-01-01",
      "updated": "2012-01-01",
      "link": "https://www.semanticscholar.org/paper/a653a99ad0a79d6833d0cde4d85806675d9819bf",
      "relevance_score": 8.0,
      "relevance_reason": "This paper is relevant as it discusses AI-driven resource allocation and optimization, which aligns with the researcher's interests in AI safety, reinforcement learning optimization, and memory allocation optimization. The methodology described in the abstract seems sound and the potential impact on improving data processing in real time is significant.",
      "author_h_indices": [
        0
      ],
      "quality_score": 8.0
    },
    {
      "arxiv_id": "s2:b2a305f92f3f0491f4dd6d2c36904901a0b81570",
      "title": "AI-Driven Cloud Resource Optimization for Smart Cities",
      "abstract": "The fast-paced advent of smart cities is generating enormous volumes of data, thus necessitating good cloud resource management for the various urban applications. This paper discusses the use of Artificial Intelligence (AI) techniques as a means of optimizing resource allocation in the cloud in smart cities, making it efficient, scalable, and sustainable. By using AI-driven techniques of reinforcement learning, predictive analytics, and deep learning, the framework of the work can automatically change the amount of resources in response to the variables of smart city applications, such as energy distribution, traffic management, and public safety, in a manner that ensures the efficient and timely delivery of these services. In particular, the study uses real-world data to evaluate the framework, thereby demonstrating the notable improvements in service quality, energy efficiency, and resource utilization. Furthermore, this research looks at issues such as the minimization of latency, cost optimization, and the inclusion of edge computing for local decision-making. The findings of the study as well as this research work indicate that $\\mathbf{A I}$ is capable of changing the cloud infrastructure into a more adaptive, intelligent, and ecofriendly source for smart cities.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/b2a305f92f3f0491f4dd6d2c36904901a0b81570",
      "relevance_score": 7.0,
      "relevance_reason": "While the paper does not specifically focus on AI safety or token expenditure optimization, it does touch upon reinforcement learning optimization and memory allocation optimization in the context of cloud resource management for smart cities, which are relevant to the researcher's interests.",
      "author_h_indices": [
        48,
        0,
        17
      ],
      "quality_score": 7.473958333333333
    },
    {
      "arxiv_id": "2505.18979",
      "title": "GhostPrompt: Jailbreaking Text-to-image Generative Models based on Dynamic Optimization",
      "abstract": "Text-to-image (T2I) generation models can inadvertently produce not-safe-for-work (NSFW) content, prompting the integration of text and image safety filters. Recent advances employ large language models (LLMs) for semantic-level detection, rendering traditional token-level perturbation attacks largely ineffective. However, our evaluation shows that existing jailbreak methods are ineffective against these modern filters. We introduce GhostPrompt, the first automated jailbreak framework that combines dynamic prompt optimization with multimodal feedback. It consists of two key components: (i) Dynamic Optimization, an iterative process that guides a large language model (LLM) using feedback from text safety filters and CLIP similarity scores to generate semantically aligned adversarial prompts; and (ii) Adaptive Safety Indicator Injection, which formulates the injection of benign visual cues as a reinforcement learning problem to bypass image-level filters. GhostPrompt achieves state-of-the-art performance, increasing the ShieldLM-7B bypass rate from 12.5\\% (Sneakyprompt) to 99.0\\%, improving CLIP score from 0.2637 to 0.2762, and reducing the time cost by $4.2 \\times$. Moreover, it generalizes to unseen filters including GPT-4.1 and successfully jailbreaks DALLE 3 to generate NSFW images in our evaluation, revealing systemic vulnerabilities in current multimodal defenses. To support further research on AI safety and red-teaming, we will release code and adversarial prompts under a controlled-access protocol.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/1be3e9092f10fc67e2b2bc84d11ab1508e0fb21b",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant to a researcher with interests in AI safety, optimization, and reinforcement learning. The methodology described in the abstract showcases novel approaches that could have potential impact within these areas.",
      "author_h_indices": [
        0,
        0,
        14,
        25,
        25
      ],
      "quality_score": 7.28
    },
    {
      "arxiv_id": "2008.10713",
      "title": "Dynamic Dispatching for Large-Scale Heterogeneous Fleet via Multi-agent Deep Reinforcement Learning",
      "abstract": "Dynamic dispatching is one of the core problems for operation optimization in traditional industries such as mining, as it is about how to smartly allocate the right resources to the right place at the right time. Conventionally, the industry relies on heuristics or even human intuitions which are often short-sighted and sub-optimal solutions. Leveraging the power of AI and Internet of Things (IoT), data-driven automation is reshaping this area. However, facing its own challenges such as large-scale and heterogenous trucks running in a highly dynamic environment, it can barely adopt methods developed in other domains (e.g., ride-sharing). In this paper, we propose a novel Deep Reinforcement Learning approach to solve the dynamic dispatching problem in mining. We first develop an event-based mining simulator with parameters calibrated in real mines. Then we propose an experience-sharing Deep Q Network with a novel abstract state/action representation to learn memories from heterogeneous agents altogether and realizes learning in a centralized way. We demonstrate that the proposed methods significantly outperform the most widely adopted approaches in the industry by 5.56% in terms of productivity. The proposed approach has great potential in a broader range of industries (e.g., manufacturing, logistics) which have a large-scale of heterogenous equipment working in a highly dynamic environment, as a general framework for dynamic resource allocation.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2020-01-01",
      "updated": "2020-01-01",
      "link": "https://www.semanticscholar.org/paper/af6b757bb3ffa18cc7b1aa794d28b384e7e14ec8",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant as it discusses reinforcement learning optimization in a dynamic resource allocation scenario, which aligns with the researcher's interests. The methodology described in the abstract seems robust and the potential impact in various industries is promising.",
      "author_h_indices": [
        5,
        8,
        8,
        14,
        6,
        16
      ],
      "quality_score": 7.207812500000001
    },
    {
      "arxiv_id": "2412.02610",
      "title": "AI-Driven Resource Allocation Framework for Microservices in Hybrid Cloud Platforms",
      "abstract": "The increasing demand for scalable, efficient resource management in hybrid cloud environments has led to the exploration of AI-driven approaches for dynamic resource allocation. This paper presents an AI-driven framework for resource allocation among microservices in hybrid cloud platforms. The framework employs reinforcement learning (RL)-based resource utilization optimization to reduce costs and improve performance. The framework integrates AI models with cloud management tools to respond to challenges of dynamic scaling and cost-efficient low-latency service delivery. The reinforcement learning model continuously adjusts provisioned resources as required by the microservices and predicts the future consumption trends to minimize both under- and over-provisioning of resources. Preliminary simulation results indicate that using AI in the provision of resources related to costs can reduce expenditure by up to 30-40% compared to manual provisioning and threshold-based auto-scaling approaches. It is also estimated that the efficiency in resource utilization is expected to improve by 20%-30% with a corresponding latency cut of 15%-20% during the peak demand periods. This study compares the AI-driven approach with existing static and rule-based resource allocation methods, demonstrating the capability of this new model to outperform them in terms of flexibility and real-time interests. The results indicate that reinforcement learning can make optimization of hybrid cloud platforms even better, offering a 25-35% improvement in cost efficiency and the power of scaling for microservice-based applications. The proposed framework is a strong and scalable solution to managing cloud resources in dynamic and performance-critical environments.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Engineering"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/25889f715ebabc9bef37b17fefc0b1afb4d5f2cb",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant as it discusses AI-driven resource allocation, which ties into AI safety and optimization, including reinforcement learning and memory allocation optimization. The methodology described in the abstract is of good quality, and the potential impact of the work on optimization and cost reduction is significant.",
      "author_h_indices": [
        10,
        7
      ],
      "quality_score": 7.1859375000000005
    },
    {
      "arxiv_id": "s2:da51558149bc75bdf58215da2cf72da2a756c84a",
      "title": "Deep Reinforcement Learning-Based Unmanned Aerial Vehicle Mobile Crowdsensing with Landing Constraints",
      "abstract": "With the development of artificial intelligence techniques, the unmanned aerial vehicle (UAV)-assisted intelligent mobile crowdsensing has attracted a lot of attention. In this work, a deep reinforcement learning (DRL)-based approach is proposed for solving the UAV trajectory planning problem under landing constraints. Firstly, a trajectory planning optimization problem with landing and other practical environmental constraints is formulated. Then, the optimization problem is further formulated as a Markov decision process problem, and a deep deterministic policy gradient (DDPG)-based DRL algorithm is proposed, where both the convolutional neural network and the long short-term memory are employed to process the spatial and temporal information of the state. The simulation results show that the proposed approach achieves high data collection rates while meeting the landing constraints as well as ensuring the safety of the UAV.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/da51558149bc75bdf58215da2cf72da2a756c84a",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant as it deals with optimization techniques in the context of UAV trajectory planning, which can be related to reinforcement learning optimization. The methodology is described clearly using DRL algorithms, and the use of neural networks and LSTMs adds depth. While not directly related to token expenditure optimization or LLM memory allocation, the concepts and techniques explored in this paper could be beneficial for researchers interested in AI safety and optimization.",
      "author_h_indices": [
        1,
        21,
        3
      ],
      "quality_score": 7.182291666666667
    },
    {
      "arxiv_id": "s2:270a5481f6448c256bee3b25f50584a705954993",
      "title": "D3T: Dual-Timescale Optimization of Task Scheduling and Thermal Management for Energy Efficient Geo-Distributed Data Centers",
      "abstract": "The surge of artificial intelligence (AI) has intensified compute-intensive tasks, sharply increasing the need for energy-efficient management in geo-distributed data centers. Existing approaches struggle to coordinate task scheduling and cooling control due to mismatched time constants, stochastic Information Technology (IT) workloads, variable renewable energy, and fluctuating electricity prices. To address these challenges, we propose D3T, a dual-timescale deep reinforcement learning (DRL) framework that jointly optimizes task scheduling and thermal management for energy-efficient geo-distributed data centers. At the fast timescale, D3T employs Deep Q-Network (DQN) to schedule tasks, reducing operational expenditure (OPEX) and task sojourn time. At the slow timescale, a QMIX-based multi-agent DRL method regulates cooling across distributed data centers by dynamically adjusting airflow rates, thereby preventing hotspots and reducing energy waste. Extensive experiments were conducted using TRNSYS with real-world traces, and the results demonstrate that, compared to baseline algorithms, D3T reduces OPEX by 13% in IT subsystems and 29% in cooling subsystems, improves power usage effectiveness (PUE) by 7%, and maintains more stable thermal safety across geo-distributed data centers.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2026-01-01",
      "updated": "2026-01-01",
      "link": "https://www.semanticscholar.org/paper/270a5481f6448c256bee3b25f50584a705954993",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant as it addresses energy efficiency and optimization in data centers, which are of interest in AI safety and reinforcement learning optimization. The methodology described in the abstract is of high quality, and the potential impact of the work is significant in the field.",
      "author_h_indices": [
        17,
        0,
        0,
        4,
        4,
        18
      ],
      "quality_score": 7.156770833333334
    },
    {
      "arxiv_id": "2602.00051",
      "title": "Distributional Reinforcement Learning for Condition-Based Maintenance of Multi-Pump Equipment",
      "abstract": "Condition-Based Maintenance (CBM) signifies a paradigm shift from reactive to proactive equipment management strategies in modern industrial systems. Conventional time-based maintenance schedules frequently engender superfluous expenditures and unanticipated equipment failures. In contrast, CBM utilizes real-time equipment condition data to enhance maintenance timing and optimize resource allocation. The present paper proposes a novel distributional reinforcement learning approach for multi-equipment CBM using Quantile Regression Deep Q-Networks (QR-DQN) with aging factor integration. The methodology employed in this study encompasses the concurrent administration of multiple pump units through three strategic scenarios. The implementation of safety-first, balanced, and cost-efficient approaches is imperative. Comprehensive experimental validation over 3,000 training episodes demonstrates significant performance improvements across all strategies. The Safety-First strategy demonstrates superior cost efficiency, with a return on investment (ROI) of 3.91, yielding 152\\% better performance than alternatives while requiring only 31\\% higher investment. The system exhibits 95.66\\% operational stability and immediate applicability to industrial environments.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2026-01-01",
      "updated": "2026-01-01",
      "link": "https://www.semanticscholar.org/paper/6dbeaea8c01d43517fdeeb0f04825d3c44cd9a6a",
      "relevance_score": 7.0,
      "relevance_reason": "The paper is relevant as it touches upon reinforcement learning optimization, which is of interest to the researcher. The methodology described in the abstract is of high quality, and the potential impact on condition-based maintenance is significant.",
      "author_h_indices": [
        5
      ],
      "quality_score": 7.109375
    }
  ]
}