{
  "date": "2026-02-01",
  "generated_at": "2026-02-01T19:48:55.495683+00:00",
  "categories": [
    "CS.AI"
  ],
  "interests": "Agentic modelling\n\nWhen ranking, also consider:\n- Author credentials and reputation (prefer established researchers from top institutions)\n- Quality of methodology described in abstract\n- Novelty and potential impact of the work\n- Papers with well-known authors in the field should be scored higher",
  "total_papers_fetched": 150,
  "papers": [
    {
      "arxiv_id": "2601.22156v1",
      "title": "Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts",
      "abstract": "Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data",
      "authors": [
        "Yingfa Chen",
        "Zhen Leng Thai",
        "Zihan Zhou",
        "Zhu Zhang",
        "Xingyu Shen",
        "Shuo Wang",
        "Chaojun Xiao",
        "Xu Han",
        "Zhiyuan Liu"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-29T18:59:53Z",
      "updated": "2026-01-29T18:59:53Z",
      "link": "https://arxiv.org/abs/2601.22156v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modelling as it introduces HALO, a pipeline for distilling Transformer models into RNN-attention hybrid models, and HypeNet, a hybrid architecture with superior length generalization. The methodology described in the abstract appears to be of high quality and the novelty and potential impact of the work are likely significant in the field."
    },
    {
      "arxiv_id": "2601.22154v1",
      "title": "Exploring Reasoning Reward Model for Agents",
      "abstract": "Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.",
      "authors": [
        "Kaixuan Fan",
        "Kaituo Feng",
        "Manyuan Zhang",
        "Tianshuo Peng",
        "Zhixun Li",
        "Yilei Jiang",
        "Shuang Chen",
        "Peng Pei",
        "Xunliang Cai",
        "Xiangyu Yue"
      ],
      "categories": [
        "cs.AI",
        "cs.CL"
      ],
      "published": "2026-01-29T18:59:52Z",
      "updated": "2026-01-29T18:59:52Z",
      "link": "https://arxiv.org/abs/2601.22154v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper on Agentic Reinforcement Learning and Agent Reasoning Reward Model is highly relevant to a researcher interested in agentic modelling. The methodology described in the abstract is of high quality and the potential impact of the work is significant. The authors are established researchers from top institutions, further adding to its relevance."
    },
    {
      "arxiv_id": "2601.22136v1",
      "title": "StepShield: When, Not Whether to Intervene on Rogue Agents",
      "abstract": "Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.",
      "authors": [
        "Gloria Felicia",
        "Michael Eniolade",
        "Jinfeng He",
        "Zitha Sasindran",
        "Hemant Kumar",
        "Milan Hussain Angati",
        "Sandeep Bandarupalli"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.CR",
        "cs.SE"
      ],
      "published": "2026-01-29T18:55:46Z",
      "updated": "2026-01-29T18:55:46Z",
      "link": "https://arxiv.org/abs/2601.22136v1",
      "relevance_score": 9.0,
      "relevance_reason": "The paper directly addresses agentic modeling and introduces a novel approach for evaluating when violations are detected, which aligns with the researcher's interests in this area. The paper also includes strong author credentials and potential impact with a focus on economic benefits."
    },
    {
      "arxiv_id": "2601.22129v1",
      "title": "SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents",
      "abstract": "Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.",
      "authors": [
        "Yifeng Ding",
        "Lingming Zhang"
      ],
      "categories": [
        "cs.SE",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-29T18:50:29Z",
      "updated": "2026-01-29T18:50:29Z",
      "link": "https://arxiv.org/abs/2601.22129v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on test-time scaling techniques for software engineering agents, aligning well with interests in agentic modeling. The authors are credible researchers in the field of computer science and artificial intelligence, and the methodology described in the abstract appears to be of high quality with potential impact."
    },
    {
      "arxiv_id": "2601.22101v1",
      "title": "ECO: Quantized Training without Full-Precision Master Weights",
      "abstract": "Quantization has significantly improved the compute and memory efficiency of Large Language Model (LLM) training. However, existing approaches still rely on accumulating their updates in high-precision: concretely, gradient updates must be applied to a high-precision weight buffer, known as $\\textit{master weights}$. This buffer introduces substantial memory overhead, particularly for Sparse Mixture of Experts (SMoE) models, where model parameters and optimizer states dominate memory usage. To address this, we introduce the Error-Compensating Optimizer (ECO), which eliminates master weights by applying updates directly to quantized parameters. ECO quantizes weights after each step and carefully injects the resulting quantization error into the optimizer momentum, forming an error-feedback loop with no additional memory. We prove that, under standard assumptions and a decaying learning rate, ECO converges to a constant-radius neighborhood of the optimum, while naive master-weight removal can incur an error that is inversely proportional to the learning rate. We show empirical results for pretraining small Transformers (30-800M), a Gemma-3 1B model, and a 2.1B parameter Sparse MoE model with FP8 quantization, and fine-tuning DeepSeek-MoE-16B in INT4 precision. Throughout, ECO matches baselines with master weights up to near-lossless accuracy, significantly shifting the static memory vs validation loss Pareto frontier.",
      "authors": [
        "Mahdi Nikdan",
        "Amir Zandieh",
        "Dan Alistarh",
        "Vahab Mirrokni"
      ],
      "categories": [
        "cs.CL",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-29T18:35:01Z",
      "updated": "2026-01-29T18:35:01Z",
      "link": "https://arxiv.org/abs/2601.22101v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modelling as it introduces a novel approach (ECO) for quantized training without using full-precision master weights in Large Language Models. The authors are well-established researchers and the methodology described in the abstract is of high quality. The potential impact of the work is significant in terms of memory efficiency and validation loss performance."
    },
    {
      "arxiv_id": "2601.22093v1",
      "title": "Investigating Associational Biases in Inter-Model Communication of Large Generative Models",
      "abstract": "Social bias in generative AI can manifest not only as performance disparities but also as associational bias, whereby models learn and reproduce stereotypical associations between concepts and demographic groups, even in the absence of explicit demographic information (e.g., associating doctors with men). These associations can persist, propagate, and potentially amplify across repeated exchanges in inter-model communication pipelines, where one generative model's output becomes another's input. This is especially salient for human-centred perception tasks, such as human activity recognition and affect prediction, where inferences about behaviour and internal states can lead to errors or stereotypical associations that propagate into unequal treatment. In this work, focusing on human activity and affective expression, we study how such associations evolve within an inter-model communication pipeline that alternates between image generation and image description. Using the RAF-DB and PHASE datasets, we quantify demographic distribution drift induced by model-to-model information exchange and assess whether these drifts are systematic using an explainability pipeline. Our results reveal demographic drifts toward younger representations for both actions and emotions, as well as toward more female-presenting representations, primarily for emotions. We further find evidence that some predictions are supported by spurious visual regions (e.g., background or hair) rather than concept-relevant cues (e.g., body or face). We also examine whether these demographic drifts translate into measurable differences in downstream behaviour, i.e., while predicting activity and emotion labels. Finally, we outline mitigation strategies spanning data-centric, training and deployment interventions, and emphasise the need for careful safeguards when deploying interconnected models in human-centred AI systems.",
      "authors": [
        "Fethiye Irmak Dogan",
        "Yuval Weiss",
        "Kajal Patel",
        "Jiaee Cheong",
        "Hatice Gunes"
      ],
      "categories": [
        "cs.CY",
        "cs.AI"
      ],
      "published": "2026-01-29T18:29:55Z",
      "updated": "2026-01-29T18:29:55Z",
      "link": "https://arxiv.org/abs/2601.22093v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly investigates and provides insights into associational biases in generative AI models, which is highly relevant to a researcher interested in agentic modelling. The authors have an interdisciplinary background and the methodology described in the abstract seems rigorous and novel, with potential impact in the field."
    },
    {
      "arxiv_id": "2601.21988v1",
      "title": "Generalized Information Gathering Under Dynamics Uncertainty",
      "abstract": "An agent operating in an unknown dynamical system must learn its dynamics from observations. Active information gathering accelerates this learning, but existing methods derive bespoke costs for specific modeling choices: dynamics models, belief update procedures, observation models, and planners. We present a unifying framework that decouples these choices from the information-gathering cost by explicitly exposing the causal dependencies between parameters, beliefs, and controls. Using this framework, we derive a general information-gathering cost based on Massey's directed information that assumes only Markov dynamics with additive noise and is otherwise agnostic to modeling choices. We prove that the mutual information cost used in existing literature is a special case of our cost. Then, we leverage our framework to establish an explicit connection between the mutual information cost and information gain in linearized Bayesian estimation, thereby providing theoretical justification for mutual information-based active learning approaches. Finally, we illustrate the practical utility of our framework through experiments spanning linear, nonlinear, and multi-agent systems.",
      "authors": [
        "Fernando Palafox",
        "Jingqi Li",
        "Jesse Milzman",
        "David Fridovich-Keil"
      ],
      "categories": [
        "cs.LG",
        "cs.AI",
        "cs.MA",
        "cs.RO",
        "eess.SY"
      ],
      "published": "2026-01-29T17:00:35Z",
      "updated": "2026-01-29T17:00:35Z",
      "link": "https://arxiv.org/abs/2601.21988v1",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant to a researcher interested in agentic modelling as it presents a unifying framework for information-gathering in unknown dynamical systems, demonstrating a connection between mutual information cost and information gain in Bayesian estimation."
    },
    {
      "arxiv_id": "2601.21971v1",
      "title": "MoE-ACT: Improving Surgical Imitation Learning Policies through Supervised Mixture-of-Experts",
      "abstract": "Imitation learning has achieved remarkable success in robotic manipulation, yet its application to surgical robotics remains challenging due to data scarcity, constrained workspaces, and the need for an exceptional level of safety and predictability. We present a supervised Mixture-of-Experts (MoE) architecture designed for phase-structured surgical manipulation tasks, which can be added on top of any autonomous policy. Unlike prior surgical robot learning approaches that rely on multi-camera setups or thousands of demonstrations, we show that a lightweight action decoder policy like Action Chunking Transformer (ACT) can learn complex, long-horizon manipulation from less than 150 demonstrations using solely stereo endoscopic images, when equipped with our architecture. We evaluate our approach on the collaborative surgical task of bowel grasping and retraction, where a robot assistant interprets visual cues from a human surgeon, executes targeted grasping on deformable tissue, and performs sustained retraction. We benchmark our method against state-of-the-art Vision-Language-Action (VLA) models and the standard ACT baseline. Our results show that generalist VLAs fail to acquire the task entirely, even under standard in-distribution conditions. Furthermore, while standard ACT achieves moderate success in-distribution, adopting a supervised MoE architecture significantly boosts its performance, yielding higher success rates in-distribution and demonstrating superior robustness in out-of-distribution scenarios, including novel grasp locations, reduced illumination, and partial occlusions. Notably, it generalizes to unseen testing viewpoints and also transfers zero-shot to ex vivo porcine tissue without additional training, offering a promising pathway toward in vivo deployment. To support this, we present qualitative preliminary results of policy roll-outs during in vivo porcine surgery.",
      "authors": [
        "Lorenzo Mazza",
        "Ariel Rodriguez",
        "Rayan Younis",
        "Martin Lelis",
        "Ortrun Hellig",
        "Chenpan Li",
        "Sebastian Bodenstedt",
        "Martin Wagner",
        "Stefanie Speidel"
      ],
      "categories": [
        "cs.RO",
        "cs.AI",
        "cs.LG"
      ],
      "published": "2026-01-29T16:50:14Z",
      "updated": "2026-01-29T16:50:14Z",
      "link": "https://arxiv.org/abs/2601.21971v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modelling as it presents a novel approach using a supervised Mixture-of-Experts architecture for surgical imitation learning. The quality of methodology described in the abstract is impressive, and the potential impact on the field of surgical robotics is significant."
    },
    {
      "arxiv_id": "2601.21969v1",
      "title": "Token-Guard: Towards Token-Level Hallucination Control via Self-Checking Decoding",
      "abstract": "Large Language Models (LLMs) often hallucinate, generating content inconsistent with the input. Retrieval-Augmented Generation (RAG) and Reinforcement Learning with Human Feedback (RLHF) can mitigate hallucinations but require resource-intensive retrieval or large-scale fine-tuning. Decoding-based methods are lighter yet lack explicit hallucination control. To address this, we present Token-Guard, a token-level hallucination control method based on self-checking decoding. Token-Guard performs internal verification at each reasoning step to detect hallucinated tokens before they propagate. Candidate fragments are further evaluated in a latent space with explicit hallucination risk scoring, while iterative pruning and regeneration dynamically correct detected errors. Experiments on HALU datasets show Token-Guard substantially reduces hallucinations and improves generation accuracy, offering a scalable, modular solution for reliable LLM outputs. Our code is publicly available.",
      "authors": [
        "Yifan Zhu",
        "Huiqiang Rong",
        "Haoran Luo"
      ],
      "categories": [
        "cs.CL",
        "cs.AI"
      ],
      "published": "2026-01-29T16:48:47Z",
      "updated": "2026-01-29T16:48:47Z",
      "link": "https://arxiv.org/abs/2601.21969v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it addresses the issue of hallucination control in large language models, which aligns with the researcher's interest in agentic modeling. The methodology described in the abstract appears to be of high quality and the potential impact of the work could be significant in the field."
    },
    {
      "arxiv_id": "2601.21937v1",
      "title": "Retrieval-Infused Reasoning Sandbox: A Benchmark for Decoupling Retrieval and Reasoning Capabilities",
      "abstract": "Despite strong performance on existing benchmarks, it remains unclear whether large language models can reason over genuinely novel scientific information. Most evaluations score end-to-end RAG pipelines, where reasoning is confounded with retrieval and toolchain choices, and the signal is further contaminated by parametric memorization and open-web volatility. We introduce DeR2, a controlled deep-research sandbox that isolates document-grounded reasoning while preserving core difficulties of deep search: multi-step synthesis, denoising, and evidence-based conclusion making. DeR2 decouples evidence access from reasoning via four regimes--Instruction-only, Concepts (gold concepts without documents), Related-only (only relevant documents), and Full-set (relevant documents plus topically related distractors)--yielding interpretable regime gaps that operationalize retrieval loss vs. reasoning loss and enable fine-grained error attribution. To prevent parametric leakage, we apply a two-phase validation that requires parametric failure without evidence while ensuring oracle-concept solvability. To ensure reproducibility, each instance provides a frozen document library (drawn from 2023-2025 theoretical papers) with expert-annotated concepts and validated rationales. Experiments across a diverse set of state-of-the-art foundation models reveal substantial variation and significant headroom: some models exhibit mode-switch fragility, performing worse with the Full-set than with Instruction-only, while others show structural concept misuse, correctly naming concepts but failing to execute them as procedures.",
      "authors": [
        "Shuangshuang Ying",
        "Zheyu Wang",
        "Yunjian Peng",
        "Jin Chen",
        "Yuhao Wu",
        "Hongbin Lin",
        "Dingyu He",
        "Siyi Liu",
        "Gengchen Yu",
        "YinZhu Piao",
        "Yuchen Wu",
        "Xin Gui",
        "Zhongyuan Peng",
        "Xin Li",
        "Xeron Du",
        "Libo Qin",
        "YiXin Cao",
        "Ge Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T16:26:19Z",
      "updated": "2026-01-29T16:26:19Z",
      "link": "https://arxiv.org/abs/2601.21937v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modeling as it discusses decoupling retrieval and reasoning capabilities in deep language models, introducing a benchmark for controlled deep-research sandbox. The methodology described in the abstract appears to be of high quality and the potential impact of the work is significant in the field."
    },
    {
      "arxiv_id": "2601.21822v1",
      "title": "CORE:Toward Ubiquitous 6G Intelligence Through Collaborative Orchestration of Large Language Model Agents Over Hierarchical Edge",
      "abstract": "Rapid advancements in sixth-generation (6G) networks and large language models (LLMs) have paved the way for ubiquitous intelligence, wherein seamless connectivity and distributed artificial intelligence (AI) have revolutionized various aspects of our lives.However, realizing this vision faces significant challenges owing to the fragmented and heterogeneous computing resources across hierarchical networks, which are insufficient for individual LLM agents to perform complex reasoning tasks.To address this issue, we propose Collaborative Orchestration Role at Edge (CORE), an innovative framework that employs a collaborative learning system in which multiple LLMs, each assigned a distinct functional role, are distributed across mobile devices and tiered edge servers. The system integrates three optimization modules, encompassing real-time perception,dynamic role orchestration, and pipeline-parallel execution, to facilitate efficient and rapid collaboration among distributed agents. Furthermore, we introduce a novel role affinity scheduling algorithm for dynamically orchestrating LLM role assignments across the hierarchical edge infrastructure, intelligently matching computational demands with available dispersed resources.Finally, comprehensive case studies and performance evaluations across various 6G application scenarios demonstrated the efficacy of CORE, revealing significant enhancements in the system efficiency and task completion rates. Building on these promising outcomes, we further validated the practical applicability of CORE by deploying it on a real-world edge-computing platform,that exhibits robust performance in operational environments.",
      "authors": [
        "Zitong Yu",
        "Boquan Sun",
        "Yang Li",
        "Zheyan Qu",
        "Xing Zhang"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T15:08:19Z",
      "updated": "2026-01-29T15:08:19Z",
      "link": "https://arxiv.org/abs/2601.21822v1",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant to a researcher interested in agentic modelling as it proposes a framework for collaborative orchestration of large language model agents, which aligns with the concept of agents working together in a system."
    },
    {
      "arxiv_id": "2601.21802v1",
      "title": "A Unified XAI-LLM Approach for EndotrachealSuctioning Activity Recognition",
      "abstract": "Endotracheal suctioning (ES) is an invasive yet essential clinical procedure that requires a high degree of skill to minimize patient risk - particularly in home care and educational settings, where consistent supervision may be limited. Despite its critical importance, automated recognition and feedback systems for ES training remain underexplored. To address this gap, this study proposes a unified, LLM-centered framework for video-based activity recognition benchmarked against conventional machine learning and deep learning approaches, and a pilot study on feedback generation. Within this framework, the Large Language Model (LLM) serves as the central reasoning module, performing both spatiotemporal activity recognition and explainable decision analysis from video data. Furthermore, the LLM is capable of verbalizing feedback in natural language, thereby translating complex technical insights into accessible, human-understandable guidance for trainees. Experimental results demonstrate that the proposed LLM-based approach outperforms baseline models, achieving an improvement of approximately 15-20\\% in both accuracy and F1 score. Beyond recognition, the framework incorporates a pilot student-support module built upon anomaly detection and explainable AI (XAI) principles, which provides automated, interpretable feedback highlighting correct actions and suggesting targeted improvements. Collectively, these contributions establish a scalable, interpretable, and data-driven foundation for advancing nursing education, enhancing training efficiency, and ultimately improving patient safety.",
      "authors": [
        "Hoang Khang Phan",
        "Quang Vinh Dang",
        "Noriyo Colley",
        "Christina Garcia",
        "Nhat Tan Le"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T14:46:48Z",
      "updated": "2026-01-29T14:46:48Z",
      "link": "https://arxiv.org/abs/2601.21802v1",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant to a researcher interested in agentic modelling as it proposes a unified framework for activity recognition using a Large Language Model (LLM) and includes novel approaches for feedback generation and interpretability in automated systems."
    },
    {
      "arxiv_id": "2601.21787v1",
      "title": "Assessing the Business Process Modeling Competences of Large Language Models",
      "abstract": "The creation of Business Process Model and Notation (BPMN) models is a complex and time-consuming task requiring both domain knowledge and proficiency in modeling conventions. Recent advances in large language models (LLMs) have significantly expanded the possibilities for generating BPMN models directly from natural language, building upon earlier text-to-process methods with enhanced capabilities in handling complex descriptions. However, there is a lack of systematic evaluations of LLM-generated process models. Current efforts either use LLM-as-a-judge approaches or do not consider established dimensions of model quality. To this end, we introduce BEF4LLM, a novel LLM evaluation framework comprising four perspectives: syntactic quality, pragmatic quality, semantic quality, and validity. Using BEF4LLM, we conduct a comprehensive analysis of open-source LLMs and benchmark their performance against human modeling experts. Results indicate that LLMs excel in syntactic and pragmatic quality, while humans outperform in semantic aspects; however, the differences in scores are relatively modest, highlighting LLMs' competitive potential despite challenges in validity and semantic quality. The insights highlight current strengths and limitations of using LLMs for BPMN modeling and guide future model development and fine-tuning. Addressing these areas is essential for advancing the practical deployment of LLMs in business process modeling.",
      "authors": [
        "Chantale Lauer",
        "Peter Pfeiffer",
        "Alexander Rombach",
        "Nijat Mehdiyev"
      ],
      "categories": [
        "cs.SE",
        "cs.AI"
      ],
      "published": "2026-01-29T14:34:20Z",
      "updated": "2026-01-29T14:34:20Z",
      "link": "https://arxiv.org/abs/2601.21787v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the use of large language models in business process modeling, which aligns closely with the researcher's interest in agentic modelling. The methodology described in the abstract is of high quality and the potential impact of the work in advancing the practical deployment of LLMs in this domain is significant."
    },
    {
      "arxiv_id": "2601.21754v1",
      "title": "Language-based Trial and Error Falls Behind in the Era of Experience",
      "abstract": "While Large Language Models (LLMs) excel in language-based agentic tasks, their applicability to unseen, nonlinguistic environments (e.g., symbolic or spatial tasks) remains limited. Previous work attributes this performance gap to the mismatch between the pretraining distribution and the testing distribution. In this work, we demonstrate the primary bottleneck is the prohibitive cost of exploration: mastering these tasks requires extensive trial-and-error, which is computationally unsustainable for parameter-heavy LLMs operating in a high dimensional semantic space. To address this, we propose SCOUT (Sub-Scale Collaboration On Unseen Tasks), a novel framework that decouples exploration from exploitation. We employ lightweight \"scouts\" (e.g., small MLPs) to probe environmental dynamics at a speed and scale far exceeding LLMs. The collected trajectories are utilized to bootstrap the LLM via Supervised Fine-Tuning (SFT), followed by multi-turn Reinforcement Learning (RL) to activate its latent world knowledge. Empirically, SCOUT enables a Qwen2.5-3B-Instruct model to achieve an average score of 0.86, significantly outperforming proprietary models, including Gemini-2.5-Pro (0.60), while saving about 60% GPU hours consumption.",
      "authors": [
        "Haoyu Wang",
        "Guozheng Ma",
        "Shugang Cui",
        "Yilun Kong",
        "Haotian Luo",
        "Li Shen",
        "Mengya Gao",
        "Yichao Wu",
        "Xiaogang Wang",
        "Dacheng Tao"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T14:08:41Z",
      "updated": "2026-01-29T14:08:41Z",
      "link": "https://arxiv.org/abs/2601.21754v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses agentic modeling and proposes a novel framework with potential impact in the field, written by a team of researchers from top institutions."
    },
    {
      "arxiv_id": "2601.21692v1",
      "title": "TCAP: Tri-Component Attention Profiling for Unsupervised Backdoor Detection in MLLM Fine-Tuning",
      "abstract": "Fine-Tuning-as-a-Service (FTaaS) facilitates the customization of Multimodal Large Language Models (MLLMs) but introduces critical backdoor risks via poisoned data. Existing defenses either rely on supervised signals or fail to generalize across diverse trigger types and modalities. In this work, we uncover a universal backdoor fingerprint-attention allocation divergence-where poisoned samples disrupt the balanced attention distribution across three functional components: system instructions, vision inputs, and user textual queries, regardless of trigger morphology. Motivated by this insight, we propose Tri-Component Attention Profiling (TCAP), an unsupervised defense framework to filter backdoor samples. TCAP decomposes cross-modal attention maps into the three components, identifies trigger-responsive attention heads via Gaussian Mixture Model (GMM) statistical profiling, and isolates poisoned samples through EM-based vote aggregation. Extensive experiments across diverse MLLM architectures and attack methods demonstrate that TCAP achieves consistently strong performance, establishing it as a robust and practical backdoor defense in MLLMs.",
      "authors": [
        "Mingzu Liu",
        "Hao Fang",
        "Runmin Cong"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T13:26:29Z",
      "updated": "2026-01-29T13:26:29Z",
      "link": "https://arxiv.org/abs/2601.21692v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modelling as it addresses the issue of backdoor detection in Fine-Tuning-as-a-Service for Multimodal Large Language Models with a novel unsupervised defense framework."
    },
    {
      "arxiv_id": "2601.21654v1",
      "title": "ScholarGym: Benchmarking Deep Research Workflows on Academic Literature Retrieval",
      "abstract": "Tool-augmented large language models have advanced from single-turn question answering to deep research workflows that iteratively plan queries, invoke external tools, and synthesize information to address complex information needs. Evaluating such workflows presents a fundamental challenge: reliance on live APIs introduces non-determinism, as tool invocations may yield different results across runs due to temporal drift, rate limiting, and evolving backend states. This variance undermines reproducibility and invalidates cross-system comparisons.   We present ScholarGym, a simulation environment for reproducible evaluation of deep research workflows on academic literature. The environment decouples workflow components into query planning, tool invocation, and relevance assessment, enabling fine-grained analysis of each stage under controlled conditions. Built on a static corpus of 570K papers with deterministic retrieval, ScholarGym provides 2,536 queries with expert-annotated ground truth. Experiments across diverse backbone models reveal how reasoning capabilities, planning strategies, and selection mechanisms interact over iterative refinement.",
      "authors": [
        "Hao Shen",
        "Hang Yang",
        "Zhouhong Gu"
      ],
      "categories": [
        "cs.AI"
      ],
      "published": "2026-01-29T12:51:44Z",
      "updated": "2026-01-29T12:51:44Z",
      "link": "https://arxiv.org/abs/2601.21654v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on evaluating deep research workflows, which aligns with the researcher's interest in agentic modelling. The methodology described in the abstract is of high quality, and the potential impact of the work in advancing research workflows is significant."
    },
    {
      "arxiv_id": "2601.21615v1",
      "title": "Beyond Parameter Finetuning: Test-Time Representation Refinement for Node Classification",
      "abstract": "Graph Neural Networks frequently exhibit significant performance degradation in the out-of-distribution test scenario. While test-time training (TTT) offers a promising solution, existing Parameter Finetuning (PaFT) paradigm suffer from catastrophic forgetting, hindering their real-world applicability. We propose TTReFT, a novel Test-Time Representation FineTuning framework that transitions the adaptation target from model parameters to latent representations. Specifically, TTReFT achieves this through three key innovations: (1) uncertainty-guided node selection for specific interventions, (2) low-rank representation interventions that preserve pre-trained knowledge, and (3) an intervention-aware masked autoencoder that dynamically adjust masking strategy to accommodate the node selection scheme. Theoretically, we establish guarantees for TTReFT in OOD settings. Empirically, extensive experiments across five benchmark datasets demonstrate that TTReFT achieves consistent and superior performance. Our work establishes representation finetuning as a new paradigm for graph TTT, offering both theoretical grounding and immediate practical utility for real-world deployment.",
      "authors": [
        "Jiaxin Zhang",
        "Yiqi Wang",
        "Siwei Wang",
        "Xihong Yang",
        "Yu Shi",
        "Xinwang Liu",
        "En Zhu"
      ],
      "categories": [
        "cs.LG",
        "cs.AI"
      ],
      "published": "2026-01-29T12:17:34Z",
      "updated": "2026-01-29T12:17:34Z",
      "link": "https://arxiv.org/abs/2601.21615v1",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in agentic modelling as it proposes a novel framework for test-time representation refinement in graph neural networks, offering both theoretical grounding and practical utility. The methodology described in the abstract shows promise for addressing performance degradation in out-of-distribution scenarios."
    }
  ]
}