{
  "date": "2026-02-01",
  "generated_at": "2026-02-01T20:50:34.777967+00:00",
  "categories": [
    "cs.AI",
    "cs.CL",
    "cs.LG"
  ],
  "interests": "machine learning, AI agents, large language models\n\nWhen ranking, also consider:\n- Author credentials and reputation (prefer established researchers from top institutions)\n- Quality of methodology described in abstract\n- Novelty and potential impact of the work\n- Papers with well-known authors in the field should be scored higher",
  "total_papers_fetched": 100,
  "papers": [
    {
      "arxiv_id": "2304.07297",
      "title": "Language Instructed Reinforcement Learning for Human-AI Coordination",
      "abstract": "One of the fundamental quests of AI is to produce agents that coordinate well with humans. This problem is challenging, especially in domains that lack high quality human behavioral data, because multi-agent reinforcement learning (RL) often converges to different equilibria from the ones that humans prefer. We propose a novel framework, instructRL, that enables humans to specify what kind of strategies they expect from their AI partners through natural language instructions. We use pretrained large language models to generate a prior policy conditioned on the human instruction and use the prior to regularize the RL objective. This leads to the RL agent converging to equilibria that are aligned with human preferences. We show that instructRL converges to human-like policies that satisfy the given instructions in a proof-of-concept environment as well as the challenging Hanabi benchmark. Finally, we show that knowing the language instruction significantly boosts human-AI coordination performance in human evaluations in Hanabi.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/bd05f81167ca3f77460f4a1da3bf5ade9febb15b",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it focuses on machine learning, AI agents, and large language models. It describes a novel framework for enhancing human-AI coordination through language instructions, which aligns with the researcher's interests. The quality of methodology, potential impact, and the use of pretrained language models make this work valuable for researchers in the field.",
      "author_h_indices": [
        16,
        64
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.36734693877551
    },
    {
      "arxiv_id": "2402.01622",
      "title": "TravelPlanner: A Benchmark for Real-World Planning with Language Agents",
      "abstract": "Planning has been part of the core pursuit for artificial intelligence since its conception, but earlier AI agents mostly focused on constrained settings because many of the cognitive substrates necessary for human-level planning have been lacking. Recently, language agents powered by large language models (LLMs) have shown interesting capabilities such as tool use and reasoning. Are these language agents capable of planning in more complex settings that are out of the reach of prior AI agents? To advance this investigation, we propose TravelPlanner, a new planning benchmark that focuses on travel planning, a common real-world planning scenario. It provides a rich sandbox environment, various tools for accessing nearly four million data records, and 1,225 meticulously curated planning intents and reference plans. Comprehensive evaluations show that the current language agents are not yet capable of handling such complex planning tasks-even GPT-4 only achieves a success rate of 0.6%. Language agents struggle to stay on task, use the right tools to collect information, or keep track of multiple constraints. However, we note that the mere possibility for language agents to tackle such a complex problem is in itself non-trivial progress. TravelPlanner provides a challenging yet meaningful testbed for future language agents.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/11155af5ccd1889277f4269f6bb349a7633554f4",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models, as it introduces a new benchmark for real-world planning with language agents and evaluates their capabilities in complex settings.",
      "author_h_indices": [
        13,
        23,
        20,
        5,
        13,
        1,
        13,
        42
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.14923469387755
    },
    {
      "arxiv_id": "2504.14110",
      "title": "System of Agentic AI for the Discovery of Metal-Organic Frameworks",
      "abstract": "Generative models and machine learning promise accelerated material discovery in MOFs for CO2 capture and water harvesting but face significant challenges navigating vast chemical spaces while ensuring synthetizability. Here, we present MOFGen, a system of Agentic AI comprising interconnected agents: a large language model that proposes novel MOF compositions, a diffusion model that generates crystal structures, quantum mechanical agents that optimize and filter candidates, and synthetic-feasibility agents guided by expert rules and machine learning. Trained on all experimentally reported MOFs and computational databases, MOFGen generated hundreds of thousands of novel MOF structures and synthesizable organic linkers. Our methodology was validated through high-throughput experiments and the successful synthesis of five\"AI-dreamt\"MOFs, representing a major step toward automated synthesizable material discovery.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/11ac1691fa7415da4f7727d74244c7d9b3ba6fce",
      "relevance_score": 9.0,
      "relevance_reason": "The paper involves machine learning, AI agents, and large language models in the context of material discovery, which aligns well with the researcher's interests.",
      "author_h_indices": [
        6,
        1,
        4,
        2,
        1,
        11,
        1,
        10,
        2,
        15,
        55,
        56,
        59,
        2,
        12
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.145102040816328
    },
    {
      "arxiv_id": "1911.02557",
      "title": "Feedback-Based Self-Learning in Large-Scale Conversational AI Agents",
      "abstract": "Today, most of the large-scale conversational AI agents such as Alexa, Siri, or Google Assistant are built using manually annotated data to train the different components of the system including Automatic Speech Recognition (ASR), Natural Language Understanding (NLU) and Entity Resolution (ER). Typically, the accuracy of the machine learning models in these components are improved by manually transcribing and annotating data. As the scope of these systems increase to cover more scenarios and domains, manual annotation to improve the accuracy of these components becomes prohibitively costly and time consuming. In this paper, we propose a system that leverages customer/system interaction feedback signals to automate learning without any manual annotation. Users of these systems tend to modify a previous query in hopes of fixing an error in the previous turn to get the right results. These reformulations, which are often preceded by defective experiences caused by either errors in ASR, NLU, ER or the application. In some cases, users may not properly formulate their requests (e.g. providing partial title of a song), but gleaning across a wider pool of users and sessions reveals the underlying recurrent patterns. Our proposed self-learning system automatically detects the errors, generate reformulations and deploys fixes to the runtime system to correct different types of errors occurring in different components of the system. In particular, we propose leveraging an absorbing Markov Chain model as a collaborative filtering mechanism in a novel attempt to mine these patterns. We show that our approach is highly scalable, and able to learn reformulations that reduce Alexa-user errors by pooling anonymized data across millions of customers. The proposed self-learning system achieves a win-loss ratio of 11.8 and effectively reduces the defect rate by more than 30% on utterance level reformulations in our production A/B tests. To the best of our knowledge, this is the first self-learning large-scale conversational AI system in production.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2019-01-01",
      "updated": "2019-01-01",
      "link": "https://www.semanticscholar.org/paper/a76b5dfe741c72b507159a0af7bfeb07256262b8",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it discusses a system that leverages customer feedback signals to automate learning in conversational AI agents without manual annotation, addressing the challenges of improving accuracy in different components of the system.",
      "author_h_indices": [
        6,
        4,
        11,
        32
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.121683673469388
    },
    {
      "arxiv_id": "2311.09835",
      "title": "ML-Bench: Evaluating Large Language Models and Agents for Machine Learning Tasks on Repository-Level Code",
      "abstract": "Despite Large Language Models (LLMs) like GPT-4 achieving impressive results in function-level code generation, they struggle with repository-scale code understanding (e.g., coming up with the right arguments for calling routines), requiring a deeper comprehension of complex file interactions. Also, recently, people have developed LLM agents that attempt to interact with repository code (e.g., compiling and evaluating its execution), prompting the need to evaluate their performance. These gaps have motivated our development of ML-Bench, a benchmark rooted in real-world programming applications that leverage existing code repositories to perform tasks. Addressing the need for LLMs to interpret long code contexts and translate instructions into precise, executable scripts, ML-Bench encompasses annotated 9,641 examples across 18 GitHub repositories, challenging LLMs to accommodate user-specified arguments and documentation intricacies effectively. To evaluate both LLMs and AI agents, two setups are employed: ML-LLM-Bench for assessing LLMs' text-to-code conversion within a predefined deployment environment, and ML-Agent-Bench for testing autonomous agents in an end-to-end task execution within a Linux sandbox environment. Our findings indicate that while GPT-4o leads with a Pass@5 rate surpassing 50%, there remains significant scope for improvement, highlighted by issues such as hallucinated outputs and difficulties with bash script generation. Notably, in the more demanding ML-Agent-Bench, GPT-4o achieves a 76.47% success rate, reflecting the efficacy of iterative action and feedback in complex task resolution. Our code, dataset, and models are available at https://github.com/gersteinlab/ML-bench.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/f537640c4e55d2a794a0af3ec60651f3820c702e",
      "relevance_score": 9.0,
      "relevance_reason": "The paper directly addresses machine learning, AI agents, and large language models in the context of code understanding and generation, which aligns well with the researcher's interests. The methodology described in the abstract is of high quality and the potential impact of the work is significant for the field.",
      "author_h_indices": [
        5,
        28,
        14,
        3,
        8,
        10,
        3,
        4,
        2,
        9,
        2,
        10,
        2,
        10,
        4,
        7,
        5,
        3,
        9,
        3,
        13,
        28,
        9,
        26,
        22,
        19
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.091130298273155
    },
    {
      "arxiv_id": "2502.14499",
      "title": "MLGym: A New Framework and Benchmark for Advancing AI Research Agents",
      "abstract": "We introduce Meta MLGym and MLGym-Bench, a new framework and benchmark for evaluating and developing LLM agents on AI research tasks. This is the first Gym environment for machine learning (ML) tasks, enabling research on reinforcement learning (RL) algorithms for training such agents. MLGym-bench consists of 13 diverse and open-ended AI research tasks from diverse domains such as computer vision, natural language processing, reinforcement learning, and game theory. Solving these tasks requires real-world AI research skills such as generating new ideas and hypotheses, creating and processing data, implementing ML methods, training models, running experiments, analyzing the results, and iterating through this process to improve on a given task. We evaluate a number of frontier large language models (LLMs) on our benchmarks such as Claude-3.5-Sonnet, Llama-3.1 405B, GPT-4o, o1-preview, and Gemini-1.5 Pro. Our MLGym framework makes it easy to add new tasks, integrate and evaluate models or agents, generate synthetic data at scale, as well as develop new learning algorithms for training agents on AI research tasks. We find that current frontier models can improve on the given baselines, usually by finding better hyperparameters, but do not generate novel hypotheses, algorithms, architectures, or substantial improvements. We open-source our framework and benchmark to facilitate future research in advancing the AI research capabilities of LLM agents.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/e87d34032cc2a77a7711a651296a8f51c9474929",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. It introduces a new framework and benchmark for evaluating and developing LLM agents on AI research tasks, which aligns directly with the researcher's interests. The authors also evaluate several frontier large language models on their benchmarks, providing insights into the performance of these models.",
      "author_h_indices": [
        8,
        9,
        2,
        8,
        5,
        1,
        2,
        9,
        2,
        1,
        26,
        7,
        5,
        2,
        41,
        2,
        28
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.085354141656664
    },
    {
      "arxiv_id": "s2:c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735",
      "title": "Benchmarking Large Language Models As AI Research Agents",
      "abstract": "",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/c14e4d299c7b2ab4b89bd6b7cbe53b3f1237b735",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of machine learning, AI agents, and large language models. The title suggests a methodology for benchmarking these models, which would be of high relevance to a researcher in this field.",
      "author_h_indices": [
        5,
        3,
        8,
        18
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.078061224489796
    },
    {
      "arxiv_id": "2408.06292",
      "title": "The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery",
      "abstract": "One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aides to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than $15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/33161a5a9b5dcb635b5a97475e6a6209a69ada7d",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it discusses the development of agents capable of conducting scientific research and discovering new knowledge using frontier large language models.",
      "author_h_indices": [
        5,
        14,
        8,
        9,
        5,
        3
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.06734693877551
    },
    {
      "arxiv_id": "2402.04268",
      "title": "ProtAgents: protein discovery via large language model multi-agent collaborations combining physics and machine learning",
      "abstract": "Designing de novo proteins beyond those found in nature holds significant promise for advancements in both scientific and engineering applications. Current methodologies for protein design often rely on AI-based models, such as surrogate models that address end-to-end problems by linking protein structure to material properties or vice versa. However, these models frequently focus on specific material objectives or structural properties, limiting their flexibility when incorporating out-of-domain knowledge into the design process or comprehensive data analysis is required. In this study, we introduce ProtAgents, a platform for de novo protein design based on Large Language Models (LLMs), where multiple AI agents with distinct capabilities collaboratively address complex tasks within a dynamic environment. The versatility in agent development allows for expertise in diverse domains, including knowledge retrieval, protein structure analysis, physics-based simulations, and results analysis. The dynamic collaboration between agents, empowered by LLMs, provides a versatile approach to tackling protein design and analysis problems, as demonstrated through diverse examples in this study. The problems of interest encompass designing new proteins, analyzing protein structures and obtaining new first-principles data \u2013 natural vibrational frequencies \u2013 via physics simulations. The concerted effort of the system allows for powerful automated and synergistic design of de novo proteins with targeted mechanical properties. The flexibility in designing the agents, on one hand, and their capacity in autonomous collaboration through the dynamic LLM-based multi-agent environment on the other hand, unleashes great potentials of LLMs in addressing multi-objective materials problems and opens up new avenues for autonomous materials discovery and design.",
      "authors": [],
      "categories": [
        "Physics",
        "Computer Science",
        "Biology",
        "Medicine"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/797597040167622a126c3206bbf94459238e2c19",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it specifically focuses on de novo protein design using large language models and AI agent collaborations.",
      "author_h_indices": [
        6,
        7
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.05969387755102
    },
    {
      "arxiv_id": "2311.18232",
      "title": "LMRL Gym: Benchmarks for Multi-Turn Reinforcement Learning with Language Models",
      "abstract": "Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2023-01-01",
      "updated": "2023-01-01",
      "link": "https://www.semanticscholar.org/paper/1e672bf4d38a93c4c140ee208216425444368fa6",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. It explores the use of reinforcement learning to create goal-directed language agents using LLMs, addressing challenges in multi-turn conversations. The methodology and benchmarks introduced are of interest for researchers in this field.",
      "author_h_indices": [
        6,
        1,
        9,
        2,
        3,
        18,
        3,
        10
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.05969387755102
    },
    {
      "arxiv_id": "2503.22164",
      "title": "PharmAgents: Building a Virtual Pharma with Large Language Model Agents",
      "abstract": "The discovery of novel small molecule drugs remains a critical scientific challenge with far-reaching implications for treating diseases and advancing human health. Traditional drug development--especially for small molecule therapeutics--is a highly complex, resource-intensive, and time-consuming process that requires multidisciplinary collaboration. Recent breakthroughs in artificial intelligence (AI), particularly the rise of large language models (LLMs), present a transformative opportunity to streamline and accelerate this process. In this paper, we introduce PharmAgents, a virtual pharmaceutical ecosystem driven by LLM-based multi-agent collaboration. PharmAgents simulates the full drug discovery workflow--from target discovery to preclinical evaluation--by integrating explainable, LLM-driven agents equipped with specialized machine learning models and computational tools. Through structured knowledge exchange and automated optimization, PharmAgents identifies potential therapeutic targets, discovers promising lead compounds, enhances binding affinity and key molecular properties, and performs in silico analyses of toxicity and synthetic feasibility. Additionally, the system supports interpretability, agent interaction, and self-evolvement, enabling it to refine future drug designs based on prior experience. By showcasing the potential of LLM-powered multi-agent systems in drug discovery, this work establishes a new paradigm for autonomous, explainable, and scalable pharmaceutical research, with future extensions toward comprehensive drug lifecycle management.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Biology"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/46e921cb6fba3423e48fa894afa9b0b47bf934d6",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it explores the use of LLM-based multi-agent systems in drug discovery, showcasing a novel application with potential impact in the field.",
      "author_h_indices": [
        6,
        6,
        2,
        3,
        9,
        3,
        12
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.053790087463558
    },
    {
      "arxiv_id": "2408.14033",
      "title": "MLR-Copilot: Autonomous Machine Learning Research based on Large Language Models Agents",
      "abstract": "Autonomous machine learning research has gained significant attention recently. We present MLR-COPILOT, an autonomous Machine Learning Research framework powered by large language model agents. The system is designed to enhance ML research productivity through automatic generation and implementation of research ideas within constraints. Our work was released in August 2024 (concurrent to AI-Scientist) and has gained notable recognition from leading projects. We further enhance our ideation with training afterwards. The framework consists of three stages: idea generation, experiment implementation, and code execution. First, existing research papers are used to generate feasible ideas and experiment plans with IdeaAgent, powered by an RL-tuned LLM. Next, ExperimentAgent leverages retrieved prototype code to convert plans into executable code with optionally retrieved candidate models and data from HuggingFace. In the final stage, ExperimentAgent runs experiments, and allows subsequent iterations of debugging and human feedback for a better chance of success with executable outcomes. We evaluate our framework on five machine learning research tasks. Experiment results demonstrate the potential of our framework to facilitate ML research progress and innovation.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/d0cd8b45949b959c316a3ed75a4683d0a70b1aa9",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant as it focuses on machine learning, AI agents, and large language models. The methodology described in the abstract is of high quality, and the potential impact of the work on advancing ML research is significant. The authors' work has gained notable recognition, adding to their credibility.",
      "author_h_indices": [
        6,
        2,
        13,
        1,
        5
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.049591836734693
    },
    {
      "arxiv_id": "2410.02958",
      "title": "AutoML-Agent: A Multi-Agent LLM Framework for Full-Pipeline AutoML",
      "abstract": "Automated machine learning (AutoML) accelerates AI development by automating tasks in the development pipeline, such as optimal model search and hyperparameter tuning. Existing AutoML systems often require technical expertise to set up complex tools, which is in general time-consuming and requires a large amount of human effort. Therefore, recent works have started exploiting large language models (LLM) to lessen such burden and increase the usability of AutoML frameworks via a natural language interface, allowing non-expert users to build their data-driven solutions. These methods, however, are usually designed only for a particular process in the AI development pipeline and do not efficiently use the inherent capacity of the LLMs. This paper proposes AutoML-Agent, a novel multi-agent framework tailored for full-pipeline AutoML, i.e., from data retrieval to model deployment. AutoML-Agent takes user's task descriptions, facilitates collaboration between specialized LLM agents, and delivers deployment-ready models. Unlike existing work, instead of devising a single plan, we introduce a retrieval-augmented planning strategy to enhance exploration to search for more optimal plans. We also decompose each plan into sub-tasks (e.g., data preprocessing and neural network design) each of which is solved by a specialized agent we build via prompting executing in parallel, making the search process more efficient. Moreover, we propose a multi-stage verification to verify executed results and guide the code generation LLM in implementing successful solutions. Extensive experiments on seven downstream tasks using fourteen datasets show that AutoML-Agent achieves a higher success rate in automating the full AutoML process, yielding systems with good performance throughout the diverse domains.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/efde8940a0b924e93d35184c4a1e8f9670b94fe7",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. The methodology described in the abstract shows a novel approach to using LLMs in the AutoML process, which could have a significant impact on the field. The authors also seem to be established researchers in the field of computer science.",
      "author_h_indices": [
        6,
        8,
        2
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.048979591836734
    },
    {
      "arxiv_id": "2501.07278",
      "title": "Lifelong Learning of Large Language Model based Agents: A Roadmap",
      "abstract": "Lifelong learning, also known as continual or incremental learning, is a crucial component for advancing Artificial General Intelligence (AGI) by enabling systems to continuously adapt in dynamic environments. While large language models (LLMs) have demonstrated impressive capabilities in natural language processing, existing LLM agents are typically designed for static systems and lack the ability to adapt over time in response to new challenges. This survey is the first to systematically summarize the potential techniques for incorporating lifelong learning into LLM-based agents. We categorize the core components of these agents into three modules: the perception module for multimodal input integration, the memory module for storing and retrieving evolving knowledge, and the action module for grounded interactions with the dynamic environment. We highlight how these pillars collectively enable continuous adaptation, mitigate catastrophic forgetting, and improve long-term performance. This survey provides a roadmap for researchers and practitioners working to develop lifelong learning capabilities in LLM agents, offering insights into emerging trends, evaluation metrics, and application scenarios. Relevant literature and resources are available at at https://github.com/qianlimalab/ awesome-lifelong-llm-agent.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Medicine"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/76aebf01bdfeaf743ac83ac231384a861f7b69ca",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it directly addresses the interests in machine learning, AI agents, and large language models. The authors provide a novel roadmap for incorporating lifelong learning into LLM-based agents, which has the potential to impact the field of AGI.",
      "author_h_indices": [
        11,
        2,
        3,
        2,
        5,
        4,
        5,
        8
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.045918367346939
    },
    {
      "arxiv_id": "s2:def4e64652e5ea1f997a9c802c8e5d430d1c8d66",
      "title": "Augmented Cognition Meets AI: Enhancing Human Performance with Real-Time, Adaptive, and Trustworthy Intelligence",
      "abstract": "This panel explores how advancements in artificial intelligence (AI) are transforming the field of augmented cognition (AC). Traditionally focused on adapting system behavior based on user state, augmented cognition is now expanding into new territory with AI technologies that can sense, interpret, and respond in real time. Emerging AI (e.g., large language models, multimodal machine learning, and generative agents) enables new modes of measurement, prediction, and interaction for AI-based augmented cognition. Panelists will highlight how AC systems differ from earlier automation approaches, offering examples from diverse application domains. Use cases include AI copilots that monitor driver fatigue and distraction, adaptive digital teammates that guide workers through complex tasks, and cognitive assistants that support rapid decision- making in high-pressure environments. The conversation will address key design considerations such as the level of human involvement, how to calibrate interaction, and what AI capabilities are still needed. They will also examine critical implementation challenges, including data privacy, cybersecurity, user trust, and ethical concerns. As AI increasingly acts as a synthetic collaborator, the panel will consider how augmented cognition is defined and applied. They will identify research gaps, propose future directions, and explore how human-AI systems can enhance safety, performance, and decision-making across complex domains.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/def4e64652e5ea1f997a9c802c8e5d430d1c8d66",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of machine learning, AI agents, and large language models. The authors discuss how AI technologies are advancing augmented cognition, which aligns closely with the researcher's interests. The methodology described in the abstract suggests a focus on real-time, adaptive, and trustworthy intelligence, which is relevant to the field. Additionally, the potential impact of the work in enhancing human performance with AI and augmented cognition is significant.",
      "author_h_indices": [
        1,
        15,
        4,
        2,
        3
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.045918367346939
    },
    {
      "arxiv_id": "2506.22598",
      "title": "RExBench: Can coding agents autonomously implement AI research extensions?",
      "abstract": "Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/c0657830bf237e714a0ffb6a168c2c7f5727f328",
      "relevance_score": 9.0,
      "relevance_reason": "This paper directly addresses the interests of machine learning, AI agents, and large language models. It presents a novel benchmark for evaluating AI agents' ability to autonomously implement research extensions, which could have a significant impact on the field.",
      "author_h_indices": [
        1,
        1,
        1,
        2,
        2,
        20
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.041326530612245
    },
    {
      "arxiv_id": "s2:56cf2311379429120ba6d8fc6de5d799c8a9da38",
      "title": "Digital Cardiovascular Twins, AI Agents, and Sensor Data: A Narrative Review from System Architecture to Proactive Heart Health",
      "abstract": "Cardiovascular disease remains the world\u2019s leading cause of mortality, yet everyday care still relies on episodic, symptom-driven interventions that detect ischemia, arrhythmias, and remodeling only after tissue damage has begun, limiting the effectiveness of therapy. A narrative review synthesized 183 studies published between 2016 and 2025 that were located through PubMed, MDPI, Scopus, IEEE Xplore, and Web of Science. This review examines CVD diagnostics using innovative technologies such as digital cardiovascular twins, which involve the collection of data from wearable IoT devices (electrocardiography (ECG), photoplethysmography (PPG), and mechanocardiography), clinical records, laboratory biomarkers, and genetic markers, as well as their integration with artificial intelligence (AI), including machine learning and deep learning, graph and transformer networks for interpreting multi-dimensional data streams and creating prognostic models, as well as generative AI, medical large language models (LLMs), and autonomous agents for decision support, personalized alerts, and treatment scenario modeling, and with cloud and edge computing for data processing. This multi-layered architecture enables the detection of silent pathologies long before clinical manifestations, transforming continuous observations into actionable recommendations and shifting cardiology from reactive treatment to predictive and preventive care. Evidence converges on four layers: sensors streaming multimodal clinical and environmental data; hybrid analytics that integrate hemodynamic models with deep-, graph- and transformer learning while Bayesian and Kalman filters manage uncertainty; decision support delivered by domain-tuned medical LLMs and autonomous agents; and prospective simulations that trial pacing or pharmacotherapy before bedside use, closing the prediction-intervention loop. This stack flags silent pathology weeks in advance and steers proactive personalized prevention. It also lays the groundwork for software-as-a-medical-device ecosystems and new regulatory guidance for trustworthy AI-enabled cardiovascular care.",
      "authors": [],
      "categories": [
        "Medicine"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/56cf2311379429120ba6d8fc6de5d799c8a9da38",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it discusses the integration of artificial intelligence with digital cardiovascular twins for proactive heart health. The methodology described in the abstract demonstrates the use of innovative technologies and multi-layered architecture, showing potential impact on predictive and preventive care in cardiology.",
      "author_h_indices": [
        5,
        5,
        4,
        4,
        5,
        2,
        1,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.030994897959184
    },
    {
      "arxiv_id": "2410.23953",
      "title": "Representative Social Choice: From Learning Theory to AI Alignment",
      "abstract": "Social choice theory is the study of preference aggregation across a population, used both in mechanism design for human agents and in the democratic alignment of language models. In this study, we propose the representative social choice framework for the modeling of democratic representation in collective decisions, where the number of issues and individuals are too large for mechanisms to consider all preferences directly. These scenarios are widespread in real-world decision-making processes, such as jury trials, legislation, corporate governance, and, more recently, language model alignment. In representative social choice, the population is represented by a finite sample of individual-issue pairs based on which social choice decisions are made. We show that many of the deepest questions in representative social choice can be formulated as statistical learning problems, and prove the generalization properties of social choice mechanisms using the theory of machine learning. We further formulate axioms for representative social choice, and prove Arrow-like impossibility theorems with new combinatorial tools of analysis. Our framework introduces the representative approach to social choice, opening up research directions at the intersection of social choice, learning theory, and AI alignment.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/f391132b08670ff5f6ead02a6fd9c87b5406cc2f",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant as it discusses the intersection of social choice theory, learning theory, and AI alignment, which aligns closely with the researcher's interests in machine learning, AI agents, and large language models. The methodology described in the abstract indicates a strong foundation in statistical learning and machine learning, and the potential impact of the work in the field is significant.",
      "author_h_indices": [
        3
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.027551020408163
    },
    {
      "arxiv_id": "2506.12822",
      "title": "Enhancing Rating-Based Reinforcement Learning to Effectively Leverage Feedback from Large Vision-Language Models",
      "abstract": "Designing effective reward functions remains a fundamental challenge in reinforcement learning (RL), as it often requires extensive human effort and domain expertise. While RL from human feedback has been successful in aligning agents with human intent, acquiring high-quality feedback is costly and labor-intensive, limiting its scalability. Recent advancements in foundation models present a promising alternative--leveraging AI-generated feedback to reduce reliance on human supervision in reward learning. Building on this paradigm, we introduce ERL-VLM, an enhanced rating-based RL method that effectively learns reward functions from AI feedback. Unlike prior methods that rely on pairwise comparisons, ERL-VLM queries large vision-language models (VLMs) for absolute ratings of individual trajectories, enabling more expressive feedback and improved sample efficiency. Additionally, we propose key enhancements to rating-based RL, addressing instability issues caused by data imbalance and noisy labels. Through extensive experiments across both low-level and high-level control tasks, we demonstrate that ERL-VLM significantly outperforms existing VLM-based reward generation methods. Our results demonstrate the potential of AI feedback for scaling RL with minimal human intervention, paving the way for more autonomous and efficient reward learning.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/8dccf45aa7b6d3e7ef8fe762751cb35c86778141",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. The authors introduce an enhanced rating-based RL method that leverages feedback from AI models to learn reward functions, a topic of interest to researchers in the field. The methodology described in the abstract appears to be of high quality, with potential impact on scaling RL with minimal human intervention.",
      "author_h_indices": [
        10,
        3,
        1,
        1,
        1,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.026020408163266
    },
    {
      "arxiv_id": "s2:8f41b210479a7bb37dde62adb3008b92622e4a61",
      "title": "AI-Augmented DevOps: Autonomous Software Delivery with Large Language Models",
      "abstract": "The evolution of DevOps has accelerated software delivery through continuous integration, deployment, and infrastructure automation. However, modern systems' scale, dynamism, and complexity have outpaced traditional automation techniques. This paper introduces AI-augmented DevOps: an architectural and operational model that embeds intelligent agents powered by Large Language Models (LLMs) and machine learning into the DevOps lifecycle. We propose a modular five-layer framework consisting of observation, inference, action, feedback, and interaction layers, each designed to support autonomous, traceable, and policy-compliant decision-making. Our implementation leverages GPT-4 and reinforcement learning to enhance tasks such as log summarization, Infrastructure-As-Code (IaC) generation, and real-time incident remediation. A simulated CI/CD environment and real-world case studies demonstrate significant improvements in deployment frequency, MTTR, and change failure rates. The paper provides a blueprint for integrating AI into software delivery pipelines, enabling systems that continuously learn, adapt, and improve while maintaining human oversight and operational governance.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/8f41b210479a7bb37dde62adb3008b92622e4a61",
      "relevance_score": 9.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models as it introduces AI-augmented DevOps using Large Language Models. The methodology described in the abstract is of high quality and the potential impact of the work is significant in the field. The use of GPT-4 and reinforcement learning adds to the novelty and credibility of the research, making it a valuable resource for researchers in this area.",
      "author_h_indices": [
        1,
        1,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.009183673469387
    },
    {
      "arxiv_id": "2512.11935",
      "title": "AGAPI-Agents: An Open-Access Agentic AI Platform for Accelerated Materials Design on AtomGPT.org",
      "abstract": "Artificial intelligence is reshaping scientific discovery, yet its use in materials research remains limited by fragmented computational ecosystems, reproducibility challenges, and dependence on commercial large language models (LLMs). Here we introduce AGAPI (AtomGPT.org API), an open-access agentic AI platform that integrates more than eight open-source LLMs with over twenty materials-science API endpoints, unifying databases, simulation tools, and machine-learning models through a common orchestration framework. AGAPI employs an Agent-Planner-Executor-Summarizer architecture that autonomously constructs and executes multi-step workflows spanning materials data retrieval, graph neural network property prediction, machine-learning force-field optimization, tight-binding calculations, diffraction analysis, and inverse design. We demonstrate AGAPI through end-to-end workflows, including heterostructure construction, powder X-ray diffraction analysis, and semiconductor defect engineering requiring up to ten sequential operations. In addition, we evaluate AGAPI using 30+ example prompts as test cases and compare agentic predictions with and without tool access against experimental data. With more than 1,000 active users, AGAPI provides a scalable and transparent foundation for reproducible, AI-accelerated materials discovery. AGAPI-Agents codebase is available at https://github.com/atomgptlab/agapi.",
      "authors": [],
      "categories": [
        "Computer Science",
        "Physics"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/b9169ae035962c109e3594074ebb17f8cdcde613",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. The methodology described in the abstract demonstrates the integration of multiple LLMs and materials-science API endpoints for materials design. The novelty and potential impact of the work, along with the reputation of the authors in the field of AI and materials science, make this paper highly relevant.",
      "author_h_indices": [
        1,
        1,
        1,
        1,
        1,
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.009183673469387
    },
    {
      "arxiv_id": "s2:8937a159efa2267fa131ac6ea4549b06db3d2fb5",
      "title": "Formal And AI Hybrid Techniques For Scalable Verification Of Large System-On-Chips",
      "abstract": "The semiconductor industry confronts escalating verification challenges as System-on-Chip designs integrate billions of transistors across heterogeneous subsystems, including artificial intelligence accelerators, central processing units, graphics processing units, and domain-specific processors. Traditional simulation-based verification struggles to provide exhaustive confidence, while formal verification encounters state-space explosion when applied to large-scale designs with millions of flip-flops and complex hierarchical interfaces. The convergence of artificial intelligence and formal verification introduces a transformative paradigm where machine learning algorithms intelligently guide formal solvers, predict verification complexity, and reuse accumulated proof knowledge to overcome scalability barriers. Graph Neural Networks enable the prediction of proof convergence based on circuit topologies, while Reinforcement Learning agents dynamically optimize solver parameters in real-time. Large Language Models trained on hardware description languages automatically generate SystemVerilog Assertions from natural language specifications. The hybrid architecture integrates artificial intelligence modules directly into formal verification loops through layered frameworks encompassing data ingestion, inference engines, formal solvers, and continuous retraining mechanisms. Industrial deployments across networking accelerators, processor subsystems, and mixed-signal controllers demonstrate substantial improvements in verification runtime, property coverage completion, and proof convergence rates. The framework uses advanced features such as intelligent cone partitioning to do parallel verification, adaptive lemma insertion to achieve faster proof convergence, constraint pruning using machine learning classifiers, and reuse of previous proof caches between design runs. Economic analysis shows high returns in terms of cost of reduced computation, increased engineering resource, fewer silicon respins, and quicker time-to-market. The combination of mathematical rigor and adaptive machine intelligence is one of the basic changes to autonomous verification ecosystems, where design evolution provides continuous concrete value, and the semiconductor industry is poised to sustain verification performance with linearly growing design complexity and to provide self-optimizing verification platforms that make steady gains as reflected by successive product generations.",
      "authors": [],
      "categories": [
        "semantic_scholar"
      ],
      "published": "2025-01-01",
      "updated": "2025-01-01",
      "link": "https://www.semanticscholar.org/paper/8937a159efa2267fa131ac6ea4549b06db3d2fb5",
      "relevance_score": 9.0,
      "relevance_reason": "The paper is highly relevant as it discusses the intersection of machine learning, AI agents, and large language models in the context of formal verification for System-on-Chip designs, which aligns closely with the researcher's interests.",
      "author_h_indices": [
        1
      ],
      "huggingface_upvotes": null,
      "quality_score": 9.009183673469387
    },
    {
      "arxiv_id": "2405.17631",
      "title": "BioDiscoveryAgent: An AI Agent for Designing Genetic Perturbation Experiments",
      "abstract": "Agents based on large language models have shown great potential in accelerating scientific discovery by leveraging their rich background knowledge and reasoning capabilities. In this paper, we introduce BioDiscoveryAgent, an agent that designs new experiments, reasons about their outcomes, and efficiently navigates the hypothesis space to reach desired solutions. We demonstrate our agent on the problem of designing genetic perturbation experiments, where the aim is to find a small subset out of many possible genes that, when perturbed, result in a specific phenotype (e.g., cell growth). Utilizing its biological knowledge, BioDiscoveryAgent can uniquely design new experiments without the need to train a machine learning model or explicitly design an acquisition function as in Bayesian optimization. Moreover, BioDiscoveryAgent, using Claude 3.5 Sonnet, achieves an average of 21% improvement in predicting relevant genetic perturbations across six datasets, and a 46% improvement in the harder task of non-essential gene perturbation, compared to existing Bayesian optimization baselines specifically trained for this task. Our evaluation includes one dataset that is unpublished, ensuring it is not part of the language model's training data. Additionally, BioDiscoveryAgent predicts gene combinations to perturb more than twice as accurately as a random baseline, a task so far not explored in the context of closed-loop experiment design. The agent also has access to tools for searching the biomedical literature, executing code to analyze biological datasets, and prompting another agent to critically evaluate its predictions. Overall, BioDiscoveryAgent is interpretable at every stage, representing an accessible new paradigm in the computational design of biological experiments with the potential to augment scientists' efficacy.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/bba1d4de76b8330bea5f73ddeb99da4e01c02e16",
      "relevance_score": 8.0,
      "relevance_reason": "Relevant paper on AI agents for experiment design, leveraging language models, with potential impact on machine learning and AI research. Authors demonstrate improvements over existing methods and include evaluation on unpublished datasets.",
      "author_h_indices": [
        16,
        3,
        5,
        1,
        1,
        8,
        147
      ],
      "huggingface_upvotes": null,
      "quality_score": 8.211078717201167
    },
    {
      "arxiv_id": "2402.05119",
      "title": "A Closer Look at the Limitations of Instruction Tuning",
      "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs) using instruction-response pairs, has emerged as the predominant method for transforming base pre-trained LLMs into open-domain conversational agents. While IT has achieved notable success and widespread adoption, its limitations and shortcomings remain underexplored. In this paper, through rigorous experiments and an in-depth analysis of the changes LLMs undergo through IT, we reveal various limitations of IT. In particular, we show that (1) IT fails to enhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning response initiation and style tokens, and full-parameter fine-tuning leads to knowledge degradation. (2) Copying response patterns from IT datasets derived from knowledgeable sources leads to a decline in response quality. (3) Full-parameter fine-tuning increases hallucination by inaccurately borrowing tokens from conceptually similar instances in the IT dataset for generating responses. (4) Popular methods to improve IT do not lead to performance improvements over a simple LoRA fine-tuned model. Our findings reveal that responses generated solely from pre-trained knowledge consistently outperform responses by models that learn any form of new knowledge from IT on open-source datasets. We hope the insights and challenges revealed in this paper inspire future work in related directions.",
      "authors": [],
      "categories": [
        "Computer Science"
      ],
      "published": "2024-01-01",
      "updated": "2024-01-01",
      "link": "https://www.semanticscholar.org/paper/acbce5ebf3f254188d10f6ba7de1ba716db89774",
      "relevance_score": 8.0,
      "relevance_reason": "This paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models. It addresses the limitations of Instruction Tuning, a key process in training LLMs for conversational agents, which aligns with the researcher's interests. The methodology described in the abstract demonstrates thorough experiments and analysis, and the potential impact of the work can inspire future research in the field.",
      "author_h_indices": [
        18,
        10,
        2,
        15,
        10,
        12,
        2,
        56,
        20
      ],
      "huggingface_upvotes": null,
      "quality_score": 8.131519274376418
    },
    {
      "arxiv_id": "2601.09027",
      "title": "Agentic AI and Machine Learning for Accelerated Materials Discovery and Applications",
      "abstract": "Artificial Intelligence (AI), especially AI agents, is increasingly being applied to chemistry, healthcare, and manufacturing to enhance productivity. In this review, we discuss the progress of AI and agentic AI in areas related to, and beyond polymer materials and discovery chemistry. More specifically, the focus is on the need for efficient discovery, core concepts, and large language models. Consequently, applications are showcased in scenarios such as (1) flow chemistry, (2) biosensors, and (3) batteries.",
      "authors": [],
      "categories": [
        "Physics"
      ],
      "published": "2026-01-01",
      "updated": "2026-01-01",
      "link": "https://www.semanticscholar.org/paper/957a8d1ddfe4304d913436999c0ba3c1e19f8d1e",
      "relevance_score": 8.0,
      "relevance_reason": "The paper is highly relevant to a researcher interested in machine learning, AI agents, and large language models, as it discusses the application of AI in materials discovery and beyond, with a focus on core concepts and potential applications.",
      "author_h_indices": [
        2,
        3,
        0,
        3,
        71
      ],
      "huggingface_upvotes": null,
      "quality_score": 8.128979591836735
    }
  ]
}